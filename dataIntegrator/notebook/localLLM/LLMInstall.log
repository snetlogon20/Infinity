done
#
# To activate this environment, use
#
#     $ conda activate py39_test
#
# To deactivate an active environment, use
#
#     $ conda deactivate


(base) C:\Users\ASUS>conda activate py39_test

(py39_test) C:\Users\ASUS>pip install transformers datasets sqlalchemy tqdm pandas
Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple
Collecting transformers
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/36/f8/1f086942bc6a044e4e68dacf6de761a45367795efd5f57ad356765691c79/transformers-4.52.3-py3-none-any.whl (10.5 MB)
     ---------------------------------------- 10.5/10.5 MB 24.2 MB/s eta 0:00:00
Collecting datasets
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/20/34/a08b0ee99715eaba118cbe19a71f7b5e2425c2718ef96007c325944a1152/datasets-3.6.0-py3-none-any.whl (491 kB)
Collecting sqlalchemy
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/b0/1f/f68c58970d80ea5a1868ca5dc965d154a3b711f9ab06376ad9840d1475b8/sqlalchemy-2.0.41-cp39-cp39-win_amd64.whl (2.1 MB)
     ---------------------------------------- 2.1/2.1 MB 19.7 MB/s eta 0:00:00
Collecting tqdm
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d0/30/dc54f88dd4a2b5dc8a0279bdd7270e735851848b762aeb1c1184ed1f6b14/tqdm-4.67.1-py3-none-any.whl (78 kB)
Collecting pandas
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/2f/49/5c30646e96c684570925b772eac4eb0a8cb0ca590fa978f56c5d3ae73ea1/pandas-2.2.3-cp39-cp39-win_amd64.whl (11.6 MB)
     ---------------------------------------- 11.6/11.6 MB 31.6 MB/s eta 0:00:00
Collecting filelock (from transformers)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/4d/36/2a115987e2d8c300a974597416d9de88f2444426de9571f4b59b2cca3acc/filelock-3.18.0-py3-none-any.whl (16 kB)
Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/3a/60/90aae898b0a9f3cd65f50718c33b1f1dbfb1527d10db754e99e14e2b0a1d/huggingface_hub-0.32.0-py3-none-any.whl (509 kB)
Collecting numpy>=1.17 (from transformers)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ea/2b/7fc9f4e7ae5b507c1a3a21f0f15ed03e794c1242ea8a242ac158beb56034/numpy-2.0.2-cp39-cp39-win_amd64.whl (15.9 MB)
     ---------------------------------------- 15.9/15.9 MB 28.6 MB/s eta 0:00:00
Collecting packaging>=20.0 (from transformers)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/20/12/38679034af332785aac8774540895e234f4d07f7545804097de4b666afd8/packaging-25.0-py3-none-any.whl (66 kB)
Collecting pyyaml>=5.1 (from transformers)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/19/87/5124b1c1f2412bb95c59ec481eaf936cd32f0fe2a7b16b97b81c4c017a6a/PyYAML-6.0.2-cp39-cp39-win_amd64.whl (162 kB)
Collecting regex!=2019.12.17 (from transformers)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/dc/7b/e59b7f7c91ae110d154370c24133f947262525b5d6406df65f23422acc17/regex-2024.11.6-cp39-cp39-win_amd64.whl (274 kB)
Collecting requests (from transformers)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/f9/9b/335f9764261e915ed497fcdeb11df5dfd6f7bf257d4a6a2a686d80da4d54/requests-2.32.3-py3-none-any.whl (64 kB)
Collecting tokenizers<0.22,>=0.21 (from transformers)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e6/b6/072a8e053ae600dcc2ac0da81a23548e3b523301a442a6ca900e92ac35be/tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)
     ---------------------------------------- 2.4/2.4 MB 27.9 MB/s eta 0:00:00
Collecting safetensors>=0.4.3 (from transformers)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/69/e2/b011c38e5394c4c18fb5500778a55ec43ad6106126e74723ffaee246f56e/safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)
Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.30.0->transformers)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/2c/a9/a7022f58e081149ec0184c31ea81dcee605e1d46380b48122e1ef94ac24e/fsspec-2025.5.0-py3-none-any.whl (196 kB)
Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0,>=0.30.0->transformers)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/8b/54/b1ae86c0973cc6f0210b53d508ca3641fb6d0c56823f288d108bc7ab3cc8/typing_extensions-4.13.2-py3-none-any.whl (45 kB)
Collecting pyarrow>=15.0.0 (from datasets)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/26/cc/1eb6a01c1bbc787f596c270c46bcd2273e35154a84afcb1d0cb4cc72457e/pyarrow-20.0.0-cp39-cp39-win_amd64.whl (25.8 MB)
     ---------------------------------------- 25.8/25.8 MB 31.4 MB/s eta 0:00:00
Collecting dill<0.3.9,>=0.3.0 (from datasets)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/c9/7a/cef76fd8438a42f96db64ddaa85280485a9c395e7df3db8158cfec1eee34/dill-0.3.8-py3-none-any.whl (116 kB)
Collecting xxhash (from datasets)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d5/87/382ef7b24917d7cf4c540ee30f29b283bc87ac5893d2f89b23ea3cdf7d77/xxhash-3.5.0-cp39-cp39-win_amd64.whl (30 kB)
Collecting multiprocess<0.70.17 (from datasets)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/da/d9/f7f9379981e39b8c2511c9e0326d212accacb82f12fbfdc1aa2ce2a7b2b6/multiprocess-0.70.16-py39-none-any.whl (133 kB)
Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.30.0->transformers)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/56/53/eb690efa8513166adef3e0669afd31e95ffde69fb3c52ec2ac7223ed6018/fsspec-2025.3.0-py3-none-any.whl (193 kB)
Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/4f/ca/3f44aabf63be958ee8ee0cb4c7ad24ea58cc73b0a73919bac9a0b4b92410/aiohttp-3.11.18-cp39-cp39-win_amd64.whl (443 kB)
Collecting greenlet>=1 (from sqlalchemy)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ff/a3/eb7713abfd0a079d24b775d01c6578afbcc6676d89508ab3cbebd5c836ea/greenlet-3.2.2-cp39-cp39-win_amd64.whl (294 kB)
Collecting colorama (from tqdm)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d1/d6/3965ed04c63042e047cb6a3e6ed1a63a35087b6a609aa3a15ed8ac56c221/colorama-0.4.6-py2.py3-none-any.whl (25 kB)
Collecting python-dateutil>=2.8.2 (from pandas)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ec/57/56b9bcc3c9c6a792fcbaf139543cee77261f3651ca9da0c93f5c1221264b/python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)
Collecting pytz>=2020.1 (from pandas)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/81/c4/34e93fe5f5429d7570ec1fa436f1986fb1f00c3e0f43a589fe2bbcd22c3f/pytz-2025.2-py2.py3-none-any.whl (509 kB)
Collecting tzdata>=2022.7 (from pandas)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/5c/23/c7abc0ca0a1526a0774eca151daeb8de62ec457e77262b66b359c3c7679e/tzdata-2025.2-py2.py3-none-any.whl (347 kB)
Collecting aiohappyeyeballs>=2.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/0f/15/5bf3b99495fb160b63f95972b81750f18f7f4e02ad051373b669d17d44f2/aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)
Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ec/6a/bc7e17a3e87a2985d3e8f4da4cd0f481060eb78fb08596c42be62c90a4d9/aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)
Collecting async-timeout<6.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/fe/ba/e2081de779ca30d473f21f5b30e0e737c438205440784c7dfc81efc2b029/async_timeout-5.0.1-py3-none-any.whl (6.2 kB)
Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/77/06/bb80f5f86020c4551da315d78b3ab75e8228f89f0162f2c3a819e407941a/attrs-25.3.0-py3-none-any.whl (63 kB)
Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/8c/a2/15db0eef508761c5f7c669b70ed4ec81af4d8ddad86d1b6ef9d6746a56b4/frozenlist-1.6.0-cp39-cp39-win_amd64.whl (120 kB)
Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d4/19/dd625207c92889c1ae7b89fcbde760d99853265cfe7ffb0826393151acd1/multidict-6.4.4-cp39-cp39-win_amd64.whl (38 kB)
Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/61/73/d64abb7bb5d18880ecfac152247c0f1a5807256ea21e4737ce3019afffeb/propcache-0.3.1-cp39-cp39-win_amd64.whl (45 kB)
Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/1c/bc/80f16fc58cb3b61b15450eaf6c874d9c984c96453d9024b9d0aa4655dac9/yarl-1.20.0-cp39-cp39-win_amd64.whl (93 kB)
Collecting idna>=2.0 (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/76/c6/c88e154df9c4e1a2a66ccf0005a88dfb2650c1dffb6f5ce603dfbd452ce3/idna-3.10-py3-none-any.whl (70 kB)
Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/b7/ce/149a00dd41f10bc29e5921b496af8b574d8413afcd5e30dfa0ed46c2cc5e/six-1.17.0-py2.py3-none-any.whl (11 kB)
Collecting charset-normalizer<4,>=2 (from requests->transformers)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/6d/24/5849d46cf4311bbf21b424c443b09b459f5b436b1558c04e45dbb7cc478b/charset_normalizer-3.4.2-cp39-cp39-win_amd64.whl (105 kB)
Collecting urllib3<3,>=1.21.1 (from requests->transformers)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/6b/11/cc635220681e93a0183390e26485430ca2c7b5f9d33b15c74c2861cb8091/urllib3-2.4.0-py3-none-any.whl (128 kB)
Collecting certifi>=2017.4.17 (from requests->transformers)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/4a/7e/3db2bd1b1f9e95f7cddca6d6e75e2f2bd9f51b1246e546d88addca0106bd/certifi-2025.4.26-py3-none-any.whl (159 kB)
Installing collected packages: pytz, xxhash, urllib3, tzdata, typing-extensions, six, safetensors, regex, pyyaml, pyarrow, propcache, packaging, numpy, idna, greenlet, fsspec, frozenlist, filelock, dill, colorama, charset-normalizer, certifi, attrs, async-timeout, aiohappyeyeballs, tqdm, sqlalchemy, requests, python-dateutil, multiprocess, multidict, aiosignal, yarl, pandas, huggingface-hub, tokenizers, aiohttp, transformers, datasets
Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 async-timeout-5.0.1 attrs-25.3.0 certifi-2025.4.26 charset-normalizer-3.4.2 colorama-0.4.6 datasets-3.6.0 dill-0.3.8 filelock-3.18.0 frozenlist-1.6.0 fsspec-2025.3.0 greenlet-3.2.2 huggingface-hub-0.32.0 idna-3.10 multidict-6.4.4 multiprocess-0.70.16 numpy-2.0.2 packaging-25.0 pandas-2.2.3 propcache-0.3.1 pyarrow-20.0.0 python-dateutil-2.9.0.post0 pytz-2025.2 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.3 safetensors-0.5.3 six-1.17.0 sqlalchemy-2.0.41 tokenizers-0.21.1 tqdm-4.67.1 transformers-4.52.3 typing-extensions-4.13.2 tzdata-2025.2 urllib3-2.4.0 xxhash-3.5.0 yarl-1.20.0

(py39_test) C:\Users\ASUS>jnote

(py39_test) C:\Users\ASUS>jupyter notebook
C:\Users\ASUS\Anaconda3\lib\site-packages\notebook\services\kernels\kernelmanager.py:19: VisibleDeprecationWarning: zmq.eventloop.minitornado is deprecated in pyzmq 14.0 and will be removed.
    Install tornado itself to use zmq with the tornado IOLoop.

  from jupyter_client.session import Session
[I 17:08:14.583 NotebookApp] [nb_conda_kernels] enabled, 3 kernels found
[I 17:08:15.175 NotebookApp] JupyterLab extension loaded from C:\Users\ASUS\Anaconda3\lib\site-packages\jupyterlab
[I 17:08:15.175 NotebookApp] JupyterLab application directory is C:\Users\ASUS\Anaconda3\share\jupyter\lab
[I 17:08:15.575 NotebookApp] [nb_conda] enabled
[I 17:08:15.576 NotebookApp] Serving notebooks from local directory: C:\Users\ASUS
[I 17:08:15.577 NotebookApp] The Jupyter Notebook is running at:
[I 17:08:15.582 NotebookApp] http://localhost:8888/?token=86915868aac0c06b192ddac62c4c1ac60e4085701b463661
[I 17:08:15.582 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 17:08:15.692 NotebookApp]

    To access the notebook, open this file in a browser:
        file:///C:/Users/ASUS/AppData/Roaming/jupyter/runtime/nbserver-26392-open.html
    Or copy and paste one of these URLs:
        http://localhost:8888/?token=86915868aac0c06b192ddac62c4c1ac60e4085701b463661
[I 17:08:53.209 NotebookApp] 302 GET /?token=86915868aac0c06b192ddac62c4c1ac60e4085701b463661 (::1) 1.00ms
[I 17:09:07.383 NotebookApp] Creating new notebook in /sklearn
[E 17:09:07.624 NotebookApp] Uncaught exception GET /notebooks/sklearn/Untitled1.ipynb?kernel_name=python3 (::1)
    HTTPServerRequest(protocol='http', host='localhost:8888', method='GET', uri='/notebooks/sklearn/Untitled1.ipynb?kernel_name=python3', version='HTTP/1.1', remote_ip='::1')
    Traceback (most recent call last):
      File "C:\Users\ASUS\Anaconda3\lib\site-packages\tornado\web.py", line 1711, in _execute
        result = method(*self.path_args, **self.path_kwargs)
      File "C:\Users\ASUS\Anaconda3\lib\site-packages\tornado\web.py", line 3208, in wrapper
        return method(self, *args, **kwargs)
      File "C:\Users\ASUS\Anaconda3\lib\site-packages\notebook\notebook\handlers.py", line 59, in get
        get_custom_frontend_exporters=get_custom_frontend_exporters
      File "C:\Users\ASUS\Anaconda3\lib\site-packages\notebook\base\handlers.py", line 462, in render_template
        return template.render(**ns)
      File "C:\Users\ASUS\Anaconda3\lib\site-packages\jinja2\environment.py", line 1295, in render
        self.environment.handle_exception()
      File "C:\Users\ASUS\Anaconda3\lib\site-packages\jinja2\environment.py", line 942, in handle_exception
        raise rewrite_traceback_stack(source=source)
      File "C:\Users\ASUS\Anaconda3\lib\site-packages\notebook\templates\notebook.html", line 1, in top-level template code
        {% extends "page.html" %}
      File "C:\Users\ASUS\Anaconda3\lib\site-packages\notebook\templates\page.html", line 154, in top-level template code
        {% block header %}
      File "C:\Users\ASUS\Anaconda3\lib\site-packages\notebook\templates\notebook.html", line 120, in block 'header'
        {% for exporter in get_custom_frontend_exporters() %}
      File "C:\Users\ASUS\Anaconda3\lib\site-packages\notebook\notebook\handlers.py", line 19, in get_custom_frontend_exporters
        from nbconvert.exporters.base import get_export_names, get_exporter
      File "C:\Users\ASUS\Anaconda3\lib\site-packages\nbconvert\__init__.py", line 7, in <module>
        from . import postprocessors
      File "C:\Users\ASUS\Anaconda3\lib\site-packages\nbconvert\postprocessors\__init__.py", line 5, in <module>
        from .serve import ServePostProcessor
      File "C:\Users\ASUS\Anaconda3\lib\site-packages\nbconvert\postprocessors\serve.py", line 19, in <module>
        class ProxyHandler(web.RequestHandler):
      File "C:\Users\ASUS\Anaconda3\lib\site-packages\nbconvert\postprocessors\serve.py", line 21, in ProxyHandler
        @web.asynchronous
    AttributeError: module 'tornado.web' has no attribute 'asynchronous'
[E 17:09:07.645 NotebookApp] {
      "Host": "localhost:8888",
      "Connection": "keep-alive",
      "Sec-Ch-Ua": "\"Not_A Brand\";v=\"8\", \"Chromium\";v=\"120\", \"Google Chrome\";v=\"120\"",
      "Sec-Ch-Ua-Mobile": "?0",
      "Sec-Ch-Ua-Platform": "\"Windows\"",
      "Upgrade-Insecure-Requests": "1",
      "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
      "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
      "Sec-Fetch-Site": "same-origin",
      "Sec-Fetch-Mode": "navigate",
      "Sec-Fetch-Dest": "document",
      "Referer": "http://localhost:8888/tree/sklearn",
      "Accept-Encoding": "gzip, deflate, br, zstd",
      "Accept-Language": "zh-CN,zh;q=0.9",
      "Cookie": "ajs_anonymous_id=bb5af2b0-763a-45dc-8aa9-cc43f83570f0; _xsrf=2|a11528ed|9baa54403ad00176a80a9ab24d78183e|1746176673; username-localhost-8889=\"2|1:0|10:1746178962|23:username-localhost-8889|44:OGFmN2FjNjU0MTBjNDJiNjg2NWU5ZTE3M2JkYjY1M2I=|1fbdf439503ecaa35623b173b98e18cd3820264eaa4ac60c887f963062b7d47c\"; username-localhost-8888=\"2|1:0|10:1748077733|23:username-localhost-8888|44:YTM4YzNjZDZlNmQzNGVjYzk0ZjE3NjFiNjkwMzhkN2I=|cdf5f1aa575c659cae920ec07ad92e1dacc24b59a591451d8313740e235482f2\""
    }
[E 17:09:07.647 NotebookApp] 500 GET /notebooks/sklearn/Untitled1.ipynb?kernel_name=python3 (::1) 194.19ms referer=http://localhost:8888/tree/sklearn
[E 17:09:13.492 NotebookApp] Uncaught exception GET /notebooks/sklearn/Heatmap%20test.ipynb (::1)
    HTTPServerRequest(protocol='http', host='localhost:8888', method='GET', uri='/notebooks/sklearn/Heatmap%20test.ipynb', version='HTTP/1.1', remote_ip='::1')
    Traceback (most recent call last):
      File "C:\Users\ASUS\Anaconda3\lib\site-packages\tornado\web.py", line 1711, in _execute
        result = method(*self.path_args, **self.path_kwargs)
      File "C:\Users\ASUS\Anaconda3\lib\site-packages\tornado\web.py", line 3208, in wrapper
        return method(self, *args, **kwargs)
      File "C:\Users\ASUS\Anaconda3\lib\site-packages\notebook\notebook\handlers.py", line 59, in get
        get_custom_frontend_exporters=get_custom_frontend_exporters
      File "C:\Users\ASUS\Anaconda3\lib\site-packages\notebook\base\handlers.py", line 462, in render_template
        return template.render(**ns)
      File "C:\Users\ASUS\Anaconda3\lib\site-packages\jinja2\environment.py", line 1295, in render
        self.environment.handle_exception()
      File "C:\Users\ASUS\Anaconda3\lib\site-packages\jinja2\environment.py", line 942, in handle_exception
        raise rewrite_traceback_stack(source=source)
      File "C:\Users\ASUS\Anaconda3\lib\site-packages\notebook\templates\notebook.html", line 1, in top-level template code
        {% extends "page.html" %}
      File "C:\Users\ASUS\Anaconda3\lib\site-packages\notebook\templates\page.html", line 154, in top-level template code
        {% block header %}
      File "C:\Users\ASUS\Anaconda3\lib\site-packages\notebook\templates\notebook.html", line 120, in block 'header'
        {% for exporter in get_custom_frontend_exporters() %}
      File "C:\Users\ASUS\Anaconda3\lib\site-packages\notebook\notebook\handlers.py", line 24, in get_custom_frontend_exporters
        exporter = get_exporter(name)()
      File "C:\Users\ASUS\Anaconda3\lib\site-packages\traitlets\traitlets.py", line 958, in __new__
        inst.setup_instance(*args, **kwargs)
      File "C:\Users\ASUS\Anaconda3\lib\site-packages\traitlets\traitlets.py", line 986, in setup_instance
        super(HasTraits, self).setup_instance(*args, **kwargs)
      File "C:\Users\ASUS\Anaconda3\lib\site-packages\traitlets\traitlets.py", line 977, in setup_instance
        value.instance_init(self)
      File "C:\Users\ASUS\Anaconda3\lib\site-packages\traitlets\traitlets.py", line 1691, in instance_init
        self._resolve_classes()
      File "C:\Users\ASUS\Anaconda3\lib\site-packages\traitlets\traitlets.py", line 1696, in _resolve_classes
        self.klass = self._resolve_string(self.klass)
      File "C:\Users\ASUS\Anaconda3\lib\site-packages\traitlets\traitlets.py", line 1507, in _resolve_string
        return import_item(string)
      File "C:\Users\ASUS\Anaconda3\lib\site-packages\traitlets\utils\importstring.py", line 34, in import_item
        module = __import__(package, fromlist=[obj])
      File "C:\Users\ASUS\Anaconda3\lib\site-packages\nbconvert\__init__.py", line 7, in <module>
        from . import postprocessors
      File "C:\Users\ASUS\Anaconda3\lib\site-packages\nbconvert\postprocessors\__init__.py", line 5, in <module>
        from .serve import ServePostProcessor
      File "C:\Users\ASUS\Anaconda3\lib\site-packages\nbconvert\postprocessors\serve.py", line 19, in <module>
        class ProxyHandler(web.RequestHandler):
      File "C:\Users\ASUS\Anaconda3\lib\site-packages\nbconvert\postprocessors\serve.py", line 21, in ProxyHandler
        @web.asynchronous
    AttributeError: module 'tornado.web' has no attribute 'asynchronous'
[E 17:09:13.506 NotebookApp] {
      "Host": "localhost:8888",
      "Connection": "keep-alive",
      "Sec-Ch-Ua": "\"Not_A Brand\";v=\"8\", \"Chromium\";v=\"120\", \"Google Chrome\";v=\"120\"",
      "Sec-Ch-Ua-Mobile": "?0",
      "Sec-Ch-Ua-Platform": "\"Windows\"",
      "Upgrade-Insecure-Requests": "1",
      "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
      "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
      "Sec-Fetch-Site": "same-origin",
      "Sec-Fetch-Mode": "navigate",
      "Sec-Fetch-User": "?1",
      "Sec-Fetch-Dest": "document",
      "Referer": "http://localhost:8888/tree/sklearn",
      "Accept-Encoding": "gzip, deflate, br, zstd",
      "Accept-Language": "zh-CN,zh;q=0.9",
      "Cookie": "ajs_anonymous_id=bb5af2b0-763a-45dc-8aa9-cc43f83570f0; _xsrf=2|a11528ed|9baa54403ad00176a80a9ab24d78183e|1746176673; username-localhost-8889=\"2|1:0|10:1746178962|23:username-localhost-8889|44:OGFmN2FjNjU0MTBjNDJiNjg2NWU5ZTE3M2JkYjY1M2I=|1fbdf439503ecaa35623b173b98e18cd3820264eaa4ac60c887f963062b7d47c\"; username-localhost-8888=\"2|1:0|10:1748077733|23:username-localhost-8888|44:YTM4YzNjZDZlNmQzNGVjYzk0ZjE3NjFiNjkwMzhkN2I=|cdf5f1aa575c659cae920ec07ad92e1dacc24b59a591451d8313740e235482f2\""
    }
[E 17:09:13.508 NotebookApp] 500 GET /notebooks/sklearn/Heatmap%20test.ipynb (::1) 420.56ms referer=http://localhost:8888/tree/sklearn
[I 17:09:24.548 NotebookApp] Interrupted...
[I 17:09:24.552 NotebookApp] Shutting down 0 kernels
终止批处理操作吗(Y/N)? y

(py39_test) C:\Users\ASUS>python
Python 3.9.21 (main, Dec 11 2024, 16:35:24) [MSC v.1929 64 bit (AMD64)] on win32
Type "help", "copyright", "credits" or "license" for more information.
>>> from datasets import load_dataset
>>>
>>> # 加载WikiSQL数据集（较小，适合演示）
>>> dataset = load_dataset("wikisql")
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 1782, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 1664, in dataset_module_factory    raise e1 from None
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 1551, in dataset_module_factory    raise ConnectionError(f"Couldn't reach '{path}' on the Hub ({e.__class__.__name__})") from e
ConnectionError: Couldn't reach 'wikisql' on the Hub (LocalEntryNotFoundError)
>>> from datasets import load_dataset
>>>
>>> dataset = load_dataset(
...     "wikisql",
...     cache_dir="./cache",  # 指定缓存路径
...     use_auth_token=False,
...     # 使用镜像源（如hf-mirror.com）
...     download_config=DownloadConfig(
...         base_url="https://hf-mirror.com/datasets",
...         resume_download=True
...     )
... )
Traceback (most recent call last):
  File "<stdin>", line 6, in <module>
NameError: name 'DownloadConfig' is not defined
>>> os.environ["HF_ENDPOINT"] = "https://mirrors.tuna.tsinghua.edu.cn/hugging-face"
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'os' is not defined
>>> import os
>>> os.environ["HF_ENDPOINT"] = "https://mirrors.tuna.tsinghua.edu.cn/hugging-face"
>>> from datasets import load_dataset
>>> dataset = load_dataset("wikisql")  # 无需修改其他代码
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 1782, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 1664, in dataset_module_factory    raise e1 from None
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 1551, in dataset_module_factory    raise ConnectionError(f"Couldn't reach '{path}' on the Hub ({e.__class__.__name__})") from e
ConnectionError: Couldn't reach 'wikisql' on the Hub (LocalEntryNotFoundError)
>>> s.environ["HF_ENDPOINT"] = "https://mirrors.tuna.tsinghua.edu.cn/hugging-face/hub"
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 's' is not defined
>>> os.environ["HF_ENDPOINT"] = "https://mirrors.tuna.tsinghua.edu.cn/hugging-face/hub"
>>> from datasets import load_dataset
>>> dataset = load_dataset("wikisql")
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 1782, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 1664, in dataset_module_factory    raise e1 from None
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 1551, in dataset_module_factory    raise ConnectionError(f"Couldn't reach '{path}' on the Hub ({e.__class__.__name__})") from e
ConnectionError: Couldn't reach 'wikisql' on the Hub (LocalEntryNotFoundError)
>>> os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
>>> dataset = load_dataset("wikisql")  # 无需修改其他代码
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 1782, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 1664, in dataset_module_factory    raise e1 from None
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 1551, in dataset_module_factory    raise ConnectionError(f"Couldn't reach '{path}' on the Hub ({e.__class__.__name__})") from e
ConnectionError: Couldn't reach 'wikisql' on the Hub (LocalEntryNotFoundError)
>>>
>>>
>>>
>>>
>>> import os
>>> os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"  # 镜像站配置不变
>>>
>>> from datasets import load_dataset
>>> dataset = load_dataset("Salesforce/wikisql")  # 使用完整路径！
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 1782, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 1664, in dataset_module_factory    raise e1 from None
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 1551, in dataset_module_factory    raise ConnectionError(f"Couldn't reach '{path}' on the Hub ({e.__class__.__name__})") from e
ConnectionError: Couldn't reach 'Salesforce/wikisql' on the Hub (LocalEntryNotFoundError)
>>> from datasets import load_dataset
>>> dataset = load_dataset("datasets/Salesforce/wikisql")  # 无需修改其他代码
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 1782, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 1670, in dataset_module_factory    raise FileNotFoundError(f"Couldn't find any data file at {relative_to_absolute_path(path)}.")
FileNotFoundError: Couldn't find any data file at C:\Users\ASUS\datasets\Salesforce\wikisql.
>>>
>>>
>>>
>>>
>>>
>>> import os
>>> os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"  # 镜像站配置不变
>>>
>>> from datasets import load_dataset
>>> dataset = load_dataset("Salesforce/wikisql")  # 使用完整路径！
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 1782, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 1664, in dataset_module_factory    raise e1 from None
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 1551, in dataset_module_factory    raise ConnectionError(f"Couldn't reach '{path}' on the Hub ({e.__class__.__name__})") from e
ConnectionError: Couldn't reach 'Salesforce/wikisql' on the Hub (LocalEntryNotFoundError)
>>> import os
>>> os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
>>> from datasets import load_dataset
>>> dataset = load_dataset("Salesforce/wikisql")
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 1782, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 1664, in dataset_module_factory
    raise e1 from None
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 1551, in dataset_module_factory
    raise ConnectionError(f"Couldn't reach '{path}' on the Hub ({e.__class__.__name__})") from e
ConnectionError: Couldn't reach 'Salesforce/wikisql' on the Hub (LocalEntryNotFoundError)
>>> exit()

(py39_test) C:\Users\ASUS>pip install --upgrade datasets
Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple
Requirement already satisfied: datasets in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (3.6.0)
Requirement already satisfied: filelock in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from datasets) (3.18.0)
Requirement already satisfied: numpy>=1.17 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from datasets) (2.0.2)
Requirement already satisfied: pyarrow>=15.0.0 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from datasets) (20.0.0)
Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from datasets) (0.3.8)
Requirement already satisfied: pandas in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from datasets) (2.2.3)
Requirement already satisfied: requests>=2.32.2 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from datasets) (2.32.3)
Requirement already satisfied: tqdm>=4.66.3 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from datasets) (4.67.1)
Requirement already satisfied: xxhash in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from datasets) (3.5.0)
Requirement already satisfied: multiprocess<0.70.17 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from datasets) (0.70.16)
Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)
Requirement already satisfied: huggingface-hub>=0.24.0 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from datasets) (0.32.0)
Requirement already satisfied: packaging in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from datasets) (25.0)
Requirement already satisfied: pyyaml>=5.1 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from datasets) (6.0.2)
Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)
Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)
Requirement already satisfied: aiosignal>=1.1.2 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)
Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)
Requirement already satisfied: attrs>=17.3.0 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)
Requirement already satisfied: frozenlist>=1.1.1 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)
Requirement already satisfied: multidict<7.0,>=4.5 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)
Requirement already satisfied: propcache>=0.2.0 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)
Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)
Requirement already satisfied: typing-extensions>=4.1.0 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from multidict<7.0,>=4.5->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.13.2)
Requirement already satisfied: idna>=2.0 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)
Requirement already satisfied: charset-normalizer<4,>=2 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from requests>=2.32.2->datasets) (3.4.2)
Requirement already satisfied: urllib3<3,>=1.21.1 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from requests>=2.32.2->datasets) (2.4.0)
Requirement already satisfied: certifi>=2017.4.17 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from requests>=2.32.2->datasets) (2025.4.26)
Requirement already satisfied: colorama in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)
Requirement already satisfied: python-dateutil>=2.8.2 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from pandas->datasets) (2.9.0.post0)
Requirement already satisfied: pytz>=2020.1 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from pandas->datasets) (2025.2)
Requirement already satisfied: tzdata>=2022.7 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from pandas->datasets) (2025.2)
Requirement already satisfied: six>=1.5 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)

(py39_test) C:\Users\ASUS>python
Python 3.9.21 (main, Dec 11 2024, 16:35:24) [MSC v.1929 64 bit (AMD64)] on win32
Type "help", "copyright", "credits" or "license" for more information.
>>> import os
>>> os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
>>> from datasets import load_dataset
>>> dataset = load_dataset("Salesforce/wikisql")
README.md: 7.80kB [00:00, ?B/s]
C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\Users\ASUS\.cache\huggingface\hub\datasets--Salesforce--wikisql. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.
To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development
  warnings.warn(message)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 1782, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 1664, in dataset_module_factory
    raise e1 from None
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 1607, in dataset_module_factory
    return HubDatasetModuleFactoryWithScript(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 1264, in get_module
    trust_remote_code = resolve_trust_remote_code(self.trust_remote_code, self.name)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 137, in resolve_trust_remote_code
    raise ValueError(
ValueError: The repository for Salesforce/wikisql contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/Salesforce/wikisql.
Please pass the argument `trust_remote_code=True` to allow custom code to be run.
>>> import os
>>> os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"  # 保持镜像配置
>>>
>>> from datasets import load_dataset
>>>
>>> # 关键：添加 trust_remote_code=True 参数
>>> dataset = load_dataset("Salesforce/wikisql", trust_remote_code=True)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 1819, in load_dataset_builder
    builder_instance: DatasetBuilder = builder_cls(
TypeError: 'NoneType' object is not callable
>>>
>>>
>>> exit()

(py39_test) C:\Users\ASUS>pip install --upgrade datasets huggingface_hub
Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple
Requirement already satisfied: datasets in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (3.6.0)
Requirement already satisfied: huggingface_hub in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (0.32.0)
Requirement already satisfied: filelock in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from datasets) (3.18.0)
Requirement already satisfied: numpy>=1.17 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from datasets) (2.0.2)
Requirement already satisfied: pyarrow>=15.0.0 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from datasets) (20.0.0)
Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from datasets) (0.3.8)
Requirement already satisfied: pandas in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from datasets) (2.2.3)
Requirement already satisfied: requests>=2.32.2 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from datasets) (2.32.3)
Requirement already satisfied: tqdm>=4.66.3 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from datasets) (4.67.1)
Requirement already satisfied: xxhash in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from datasets) (3.5.0)
Requirement already satisfied: multiprocess<0.70.17 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from datasets) (0.70.16)
Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)
Requirement already satisfied: packaging in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from datasets) (25.0)
Requirement already satisfied: pyyaml>=5.1 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from datasets) (6.0.2)
Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)
Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from huggingface_hub) (4.13.2)
Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)
Requirement already satisfied: aiosignal>=1.1.2 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)
Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)
Requirement already satisfied: attrs>=17.3.0 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)
Requirement already satisfied: frozenlist>=1.1.1 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)
Requirement already satisfied: multidict<7.0,>=4.5 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)
Requirement already satisfied: propcache>=0.2.0 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)
Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)
Requirement already satisfied: idna>=2.0 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)
Requirement already satisfied: charset-normalizer<4,>=2 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from requests>=2.32.2->datasets) (3.4.2)
Requirement already satisfied: urllib3<3,>=1.21.1 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from requests>=2.32.2->datasets) (2.4.0)
Requirement already satisfied: certifi>=2017.4.17 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from requests>=2.32.2->datasets) (2025.4.26)
Requirement already satisfied: colorama in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)
Requirement already satisfied: python-dateutil>=2.8.2 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from pandas->datasets) (2.9.0.post0)
Requirement already satisfied: pytz>=2020.1 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from pandas->datasets) (2025.2)
Requirement already satisfied: tzdata>=2022.7 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from pandas->datasets) (2025.2)
Requirement already satisfied: six>=1.5 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)

(py39_test) C:\Users\ASUS>py39
'py39' 不是内部或外部命令，也不是可运行的程序
或批处理文件。

(py39_test) C:\Users\ASUS>python
Python 3.9.21 (main, Dec 11 2024, 16:35:24) [MSC v.1929 64 bit (AMD64)] on win32
Type "help", "copyright", "credits" or "license" for more information.
>>> import os
>>> os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
>>>
>>> from datasets import load_dataset
>>>
>>> # 指定版本（如 "refs/convert/parquet" 是官方维护的稳定分支）
>>> dataset = load_dataset(
...     "Salesforce/wikisql",
...     trust_remote_code=True,
...     revision="refs/convert/parquet"
... )
0000.parquet: 100%|█████████████████████████████████████████████████████████████████████| 25.2M/25.2M [00:19<00:00, 1.26MB/s]
C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\Users\ASUS\.cache\huggingface\hub\datasets--Salesforce--wikisql. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.
To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development
  warnings.warn(message)
0000.parquet: 100%|█████████████████████████████████████████████████████████████████████| 3.63M/3.63M [00:01<00:00, 1.88MB/s]
0000.parquet: 100%|█████████████████████████████████████████████████████████████████████| 7.71M/7.71M [00:04<00:00, 1.83MB/s]
Generating train split: 56355 examples [00:00, 104008.12 examples/s]
Generating validation split: 8421 examples [00:00, 98606.44 examples/s]
Generating test split: 15878 examples [00:00, 102743.90 examples/s]
>>> print(dataset["train"][0])
{'phase': 1, 'question': 'Tell me what the notes are for South Australia ', 'table': {'header': ['State/territory', 'Text/background colour', 'Format', 'Current slogan', 'Current series', 'Notes'], 'page_title': '', 'page_id': '', 'types': ['text', 'text', 'text', 'text', 'text', 'text'], 'id': '1-1000181-1', 'section_title': '', 'caption': '', 'rows': [['Australian Capital Territory', 'blue/white', 'Yaa·nna', 'ACT · CELEBRATION OF A CENTURY 2013', 'YIL·00A', 'Slogan screenprinted on plate'], ['New South Wales', 'black/yellow', 'aa·nn·aa', 'NEW SOUTH WALES', 'BX·99·HI', 'No slogan on current series'], ['New South Wales', 'black/white', 'aaa·nna', 'NSW', 'CPX·12A', 'Optional white slimline series'], ['Northern Territory', 'ochre/white', 'Ca·nn·aa', 'NT  · OUTBACK AUSTRALIA', 'CB·06·ZZ', 'New series began in June 2011'], ['Queensland', 'maroon/white', 'nnn·aaa', 'QUEENSLAND · SUNSHINE STATE', '999·TLG', 'Slogan embossed on plate'], ['South Australia', 'black/white', 'Snnn·aaa', 'SOUTH AUSTRALIA', 'S000·AZD', 'No slogan on current series'], ['Victoria', 'blue/white', 'aaa·nnn', 'VICTORIA - THE PLACE TO BE', 'ZZZ·562', 'Current series will be exhausted this year']], 'name': 'table_1000181_1'}, 'sql': {'human_readable': 'SELECT Notes FROM table WHERE Current slogan = SOUTH AUSTRALIA', 'sel': 5, 'agg': 0, 'conds': {'column_index': [3], 'operator_index': [0], 'condition': ['SOUTH AUSTRALIA']}}}
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
>>> exit()

(py39_test) C:\Users\ASUS>pip install torch==2.1.0+cpu -i https://pypi.tuna.tsinghua.edu.cn/simple
Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple
ERROR: Could not find a version that satisfies the requirement torch==2.1.0+cpu (from versions: 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2, 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0, 2.7.0)
ERROR: No matching distribution found for torch==2.1.0+cpu

(py39_test) C:\Users\ASUS>pip install torch==2.1.0 --extra-index-url https://download.pytorch.org/whl/cpu -i https://pypi.tuna.tsinghua.edu.cn/simple
Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple, https://download.pytorch.org/whl/cpu
Collecting torch==2.1.0
  Downloading https://download.pytorch.org/whl/cpu/torch-2.1.0%2Bcpu-cp39-cp39-win_amd64.whl (194.3 MB)
     ---------------------------------------- 194.3/194.3 MB 19.4 MB/s eta 0:00:00
Requirement already satisfied: filelock in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from torch==2.1.0) (3.18.0)
Requirement already satisfied: typing-extensions in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from torch==2.1.0) (4.13.2)
Collecting sympy (from torch==2.1.0)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/a2/09/77d55d46fd61b4a135c444fc97158ef34a095e5681d0a6c10b75bf356191/sympy-1.14.0-py3-none-any.whl (6.3 MB)
     ---------------------------------------- 6.3/6.3 MB 19.4 MB/s eta 0:00:00
Collecting networkx (from torch==2.1.0)
  Downloading https://download.pytorch.org/whl/networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)
Collecting jinja2 (from torch==2.1.0)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/62/a1/3d680cbfd5f4b8f15abc1d571870c5fc3e594bb582bc3b64ea099db13e56/jinja2-3.1.6-py3-none-any.whl (134 kB)
Requirement already satisfied: fsspec in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from torch==2.1.0) (2025.3.0)
Collecting MarkupSafe>=2.0 (from jinja2->torch==2.1.0)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/b3/73/085399401383ce949f727afec55ec3abd76648d04b9f22e1c0e99cb4bec3/MarkupSafe-3.0.2-cp39-cp39-win_amd64.whl (15 kB)
Collecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.1.0)
  Downloading https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)
     ---------------------------------------- 536.2/536.2 kB 18.3 MB/s eta 0:00:00
Downloading https://download.pytorch.org/whl/networkx-3.2.1-py3-none-any.whl (1.6 MB)
   ---------------------------------------- 1.6/1.6 MB 29.5 MB/s eta 0:00:00
Installing collected packages: mpmath, sympy, networkx, MarkupSafe, jinja2, torch
Successfully installed MarkupSafe-3.0.2 jinja2-3.1.6 mpmath-1.3.0 networkx-3.2.1 sympy-1.14.0 torch-2.1.0+cpu

(py39_test) C:\Users\ASUS>
(py39_test) C:\Users\ASUS>python
Python 3.9.21 (main, Dec 11 2024, 16:35:24) [MSC v.1929 64 bit (AMD64)] on win32
Type "help", "copyright", "credits" or "license" for more information.
>>> import torch

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "<stdin>", line 1, in <module>
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\torch\__init__.py", line 1382, in <module>
    from .functional import *  # noqa: F403
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\torch\functional.py", line 7, in <module>
    import torch.nn.functional as F
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\torch\nn\__init__.py", line 1, in <module>
    from .modules import *  # noqa: F403
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\torch\nn\modules\__init__.py", line 35, in <module>
    from .transformer import TransformerEncoder, TransformerDecoder, \
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\torch\nn\modules\transformer.py", line 20, in <module>
    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\torch\nn\modules\transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\torch\csrc\utils\tensor_numpy.cpp:84.)
  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
>>> print(f"PyTorch 版本: {torch.__version__}")  # 应输出 2.1.0
PyTorch 版本: 2.1.0+cpu
>>> print(f"是否为 CPU 版本: {not torch.cuda.is_available()}")  # 输出 True（无 GPU 时）
是否为 CPU 版本: True
>>> exit()

(py39_test) C:\Users\ASUS>pip uninstall numpy -y
Found existing installation: numpy 2.0.2
Uninstalling numpy-2.0.2:
  Successfully uninstalled numpy-2.0.2

(py39_test) C:\Users\ASUS>pip install numpy==1.26.0 -i https://pypi.tuna.tsinghua.edu.cn/simple
Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple
Collecting numpy==1.26.0
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/97/43/4cd9dc8c051537ed0613fcfc4229dfb9eb39fe058c8d42632977465bfdb5/numpy-1.26.0-cp39-cp39-win_amd64.whl (15.8 MB)
     ---------------------------------------- 15.8/15.8 MB 4.2 MB/s eta 0:00:00
Installing collected packages: numpy
Successfully installed numpy-1.26.0

(py39_test) C:\Users\ASUS>import numpy as np
'import' 不是内部或外部命令，也不是可运行的程序
或批处理文件。

(py39_test) C:\Users\ASUS>print(f"NumPy 版本: {np.__version__}")  # 应输出 1.26.0
无法初始化设备 PRN

(py39_test) C:\Users\ASUS>python
Python 3.9.21 (main, Dec 11 2024, 16:35:24) [MSC v.1929 64 bit (AMD64)] on win32
Type "help", "copyright", "credits" or "license" for more information.
>>> import numpy as np
>>> print(f"NumPy 版本: {np.__version__}")  # 应输出 1.26.0
NumPy 版本: 1.26.0
>>> import torch
>>> print(f"PyTorch 版本: {torch.__version__}")  # 输出 2.1.0+cpu
PyTorch 版本: 2.1.0+cpu
>>> print(f"是否为 CPU 版本: {not torch.cuda.is_available()}")  # 输出 True（无 GPU 时）
是否为 CPU 版本: True
>>> exit
Use exit() or Ctrl-Z plus Return to exit
>>>
>>>
>>>
>>>
>>> exit()

(py39_test) C:\Users\ASUS>
(py39_test) C:\Users\ASUS>python
Python 3.9.21 (main, Dec 11 2024, 16:35:24) [MSC v.1929 64 bit (AMD64)] on win32
Type "help", "copyright", "credits" or "license" for more information.
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration
>>> model_name = "t5-small"
>>> tokenizer = T5Tokenizer.from_pretrained(model_name)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\utils\import_utils.py", line 1885, in __getattribute__
    requires_backends(cls, cls._backends)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\utils\import_utils.py", line 1871, in requires_backends
    raise ImportError("".join(failed))
ImportError:
T5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.

>>> exit()

(py39_test) C:\Users\ASUS>pip install sentencepiece -i https://pypi.tuna.tsinghua.edu.cn/simple
Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple
Collecting sentencepiece
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/4b/36/497e6407700efd6b97f81bc160913a70d33b9b09227429f68fc86f387bbe/sentencepiece-0.2.0-cp39-cp39-win_amd64.whl (991 kB)
     ---------------------------------------- 991.5/991.5 kB 6.7 MB/s eta 0:00:00
Installing collected packages: sentencepiece
Successfully installed sentencepiece-0.2.0

(py39_test) C:\Users\ASUS>python
Python 3.9.21 (main, Dec 11 2024, 16:35:24) [MSC v.1929 64 bit (AMD64)] on win32
Type "help", "copyright", "credits" or "license" for more information.
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration
>>>
>>> model_name = "t5-small"
>>> tokenizer = T5Tokenizer.from_pretrained(model_name)  # 应正常加载
Traceback (most recent call last):
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connection.py", line 198, in _new_conn
    sock = connection.create_connection(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    raise err
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
TimeoutError: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    response = self._make_request(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connectionpool.py", line 488, in _make_request
    raise new_e
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connectionpool.py", line 464, in _make_request
    self._validate_conn(conn)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connectionpool.py", line 1093, in _validate_conn
    conn.connect()
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connection.py", line 704, in connect
    self.sock = sock = self._new_conn()
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x000002BE217DFBE0>: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\requests\adapters.py", line 667, in send
    resp = conn.urlopen(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connectionpool.py", line 841, in urlopen
    retries = retries.increment(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\util\retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/t5-small/tree/main/additional_chat_templates?recursive=False&expand=False (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002BE217DFBE0>: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正 确答复或连接的主机没有反应，连接尝试失败。'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\tokenization_utils_base.py", line 1968, in from_pretrained
    for template in list_repo_templates(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\utils\hub.py", line 161, in list_repo_templates
    return [
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\utils\hub.py", line 161, in <listcomp>
    return [
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\hf_api.py", line 3140, in list_repo_tree
    for path_info in paginate(path=tree_url, headers=headers, params={"recursive": recursive, "expand": expand}):
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\utils\_pagination.py", line 36, in paginate
    r = session.get(path, params=params, headers=headers)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\requests\sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\requests\sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\requests\sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\utils\_http.py", line 96, in send
    return super().send(request, *args, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\requests\adapters.py", line 700, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: (MaxRetryError("HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/t5-small/tree/main/additional_chat_templates?recursive=False&expand=False (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002BE217DFBE0>: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。'))"), '(Request ID: 23c62ff4-fd8e-4828-a563-a66ad89de4a4)')
>>> os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'os' is not defined
>>> import os
>>> os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
>>>
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration
>>>
>>> model_name = "t5-small"
>>> tokenizer = T5Tokenizer.from_pretrained(model_name)
Traceback (most recent call last):
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connection.py", line 198, in _new_conn
    sock = connection.create_connection(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    raise err
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
TimeoutError: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    response = self._make_request(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connectionpool.py", line 488, in _make_request
    raise new_e
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connectionpool.py", line 464, in _make_request
    self._validate_conn(conn)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connectionpool.py", line 1093, in _validate_conn
    conn.connect()
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connection.py", line 704, in connect
    self.sock = sock = self._new_conn()
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x000002BE217DF0D0>: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\requests\adapters.py", line 667, in send
    resp = conn.urlopen(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connectionpool.py", line 841, in urlopen
    retries = retries.increment(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\util\retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/t5-small/tree/main/additional_chat_templates?recursive=False&expand=False (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002BE217DF0D0>: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正 确答复或连接的主机没有反应，连接尝试失败。'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\tokenization_utils_base.py", line 1968, in from_pretrained
    for template in list_repo_templates(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\utils\hub.py", line 161, in list_repo_templates
    return [
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\utils\hub.py", line 161, in <listcomp>
    return [
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\hf_api.py", line 3140, in list_repo_tree
    for path_info in paginate(path=tree_url, headers=headers, params={"recursive": recursive, "expand": expand}):
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\utils\_pagination.py", line 36, in paginate
    r = session.get(path, params=params, headers=headers)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\requests\sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\requests\sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\requests\sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\utils\_http.py", line 96, in send
    return super().send(request, *args, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\requests\adapters.py", line 700, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: (MaxRetryError("HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/t5-small/tree/main/additional_chat_templates?recursive=False&expand=False (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002BE217DF0D0>: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。'))"), '(Request ID: 45e0d4b4-7d52-416c-83a0-1392ca881a1f)')
>>> exit()

(py39_test) C:\Users\ASUS>pip install --upgrade huggingface_hub
Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple
Requirement already satisfied: huggingface_hub in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (0.32.0)
Requirement already satisfied: filelock in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from huggingface_hub) (3.18.0)
Requirement already satisfied: fsspec>=2023.5.0 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from huggingface_hub) (2025.3.0)
Requirement already satisfied: packaging>=20.9 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from huggingface_hub) (25.0)
Requirement already satisfied: pyyaml>=5.1 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from huggingface_hub) (6.0.2)
Requirement already satisfied: requests in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from huggingface_hub) (2.32.3)
Requirement already satisfied: tqdm>=4.42.1 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from huggingface_hub) (4.67.1)
Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from huggingface_hub) (4.13.2)
Requirement already satisfied: colorama in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)
Requirement already satisfied: charset-normalizer<4,>=2 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from requests->huggingface_hub) (3.4.2)
Requirement already satisfied: idna<4,>=2.5 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from requests->huggingface_hub) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from requests->huggingface_hub) (2.4.0)
Requirement already satisfied: certifi>=2017.4.17 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from requests->huggingface_hub) (2025.4.26)

(py39_test) C:\Users\ASUS>python
Python 3.9.21 (main, Dec 11 2024, 16:35:24) [MSC v.1929 64 bit (AMD64)] on win32
Type "help", "copyright", "credits" or "license" for more information.
>>> import os
>>> from huggingface_hub import HfFolder, Repository, get_hf_file
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
ImportError: cannot import name 'get_hf_file' from 'huggingface_hub' (C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\__init__.py)
>>>
>>> # 设置镜像站为 hf-mirror.com
>>> os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
>>> os.environ["HUGGINGFACE_HUB_URL"] = "https://hf-mirror.com"  # 强制覆盖官方 URL
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration
>>>
>>> model_name = "t5-small"
>>> tokenizer = T5Tokenizer.from_pretrained(model_name)  # 此时应通过镜像站加载
Traceback (most recent call last):
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connection.py", line 198, in _new_conn
    sock = connection.create_connection(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    raise err
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
TimeoutError: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    response = self._make_request(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connectionpool.py", line 488, in _make_request
    raise new_e
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connectionpool.py", line 464, in _make_request
    self._validate_conn(conn)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connectionpool.py", line 1093, in _validate_conn
    conn.connect()
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connection.py", line 704, in connect
    self.sock = sock = self._new_conn()
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x000001B605159490>: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\requests\adapters.py", line 667, in send
    resp = conn.urlopen(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connectionpool.py", line 841, in urlopen
    retries = retries.increment(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\util\retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/t5-small/tree/main/additional_chat_templates?recursive=False&expand=False (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001B605159490>: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正 确答复或连接的主机没有反应，连接尝试失败。'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\tokenization_utils_base.py", line 1968, in from_pretrained
    for template in list_repo_templates(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\utils\hub.py", line 161, in list_repo_templates
    return [
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\utils\hub.py", line 161, in <listcomp>
    return [
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\hf_api.py", line 3140, in list_repo_tree
    for path_info in paginate(path=tree_url, headers=headers, params={"recursive": recursive, "expand": expand}):
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\utils\_pagination.py", line 36, in paginate
    r = session.get(path, params=params, headers=headers)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\requests\sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\requests\sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\requests\sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\utils\_http.py", line 96, in send
    return super().send(request, *args, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\requests\adapters.py", line 700, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: (MaxRetryError("HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/t5-small/tree/main/additional_chat_templates?recursive=False&expand=False (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001B605159490>: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。'))"), '(Request ID: 2cdcec19-f74d-43fe-b5ee-6c4c6eac92e3)')
>>> exit()

(py39_test) C:\Users\ASUS>pip install --upgrade huggingface_hub
Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple
Requirement already satisfied: huggingface_hub in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (0.32.0)
Requirement already satisfied: filelock in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from huggingface_hub) (3.18.0)
Requirement already satisfied: fsspec>=2023.5.0 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from huggingface_hub) (2025.3.0)
Requirement already satisfied: packaging>=20.9 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from huggingface_hub) (25.0)
Requirement already satisfied: pyyaml>=5.1 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from huggingface_hub) (6.0.2)
Requirement already satisfied: requests in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from huggingface_hub) (2.32.3)
Requirement already satisfied: tqdm>=4.42.1 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from huggingface_hub) (4.67.1)
Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from huggingface_hub) (4.13.2)
Requirement already satisfied: colorama in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)
Requirement already satisfied: charset-normalizer<4,>=2 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from requests->huggingface_hub) (3.4.2)
Requirement already satisfied: idna<4,>=2.5 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from requests->huggingface_hub) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from requests->huggingface_hub) (2.4.0)
Requirement already satisfied: certifi>=2017.4.17 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from requests->huggingface_hub) (2025.4.26)

(py39_test) C:\Users\ASUS>python
Python 3.9.21 (main, Dec 11 2024, 16:35:24) [MSC v.1929 64 bit (AMD64)] on win32
Type "help", "copyright", "credits" or "license" for more information.
>>> import os
>>> from huggingface_hub import HfFolder, Repository, get_hf_file
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
ImportError: cannot import name 'get_hf_file' from 'huggingface_hub' (C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\__init__.py)
>>>
>>> # 设置镜像站为 hf-mirror.com
>>> os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
>>> os.environ["HUGGINGFACE_HUB_URL"] = "https://hf-mirror.com"  # 强制覆盖官方 URL
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration
>>>
>>> model_name = "t5-small"
>>> tokenizer = T5Tokenizer.from_pretrained(model_name)  # 此时应通过镜像站加载
Traceback (most recent call last):
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connection.py", line 198, in _new_conn
    sock = connection.create_connection(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    raise err
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
TimeoutError: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    response = self._make_request(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connectionpool.py", line 488, in _make_request
    raise new_e
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connectionpool.py", line 464, in _make_request
    self._validate_conn(conn)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connectionpool.py", line 1093, in _validate_conn
    conn.connect()
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connection.py", line 704, in connect
    self.sock = sock = self._new_conn()
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x000001FCFAEF8490>: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\requests\adapters.py", line 667, in send
    resp = conn.urlopen(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connectionpool.py", line 841, in urlopen
    retries = retries.increment(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\util\retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/t5-small/tree/main/additional_chat_templates?recursive=False&expand=False (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001FCFAEF8490>: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正 确答复或连接的主机没有反应，连接尝试失败。'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\tokenization_utils_base.py", line 1968, in from_pretrained
    for template in list_repo_templates(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\utils\hub.py", line 161, in list_repo_templates
    return [
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\utils\hub.py", line 161, in <listcomp>
    return [
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\hf_api.py", line 3140, in list_repo_tree
    for path_info in paginate(path=tree_url, headers=headers, params={"recursive": recursive, "expand": expand}):
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\utils\_pagination.py", line 36, in paginate
    r = session.get(path, params=params, headers=headers)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\requests\sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\requests\sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\requests\sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\utils\_http.py", line 96, in send
    return super().send(request, *args, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\requests\adapters.py", line 700, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: (MaxRetryError("HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/t5-small/tree/main/additional_chat_templates?recursive=False&expand=False (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001FCFAEF8490>: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。'))"), '(Request ID: cb0c3d72-de64-4ab2-9c29-3bf92291ec6a)')
>>> exit()

(py39_test) C:\Users\ASUS>pip install --upgrade huggingface_hub>=0.26.0

(py39_test) C:\Users\ASUS>python
Python 3.9.21 (main, Dec 11 2024, 16:35:24) [MSC v.1929 64 bit (AMD64)] on win32
Type "help", "copyright", "credits" or "license" for more information.
>>> import os
>>> os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
>>> os.environ["HUGGINGFACE_HUB_BASE_URL"] = "https://hf-mirror.com"  # 关键！覆盖内部默认 URL
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration
>>>
>>> model_name = "t5-small"
>>> tokenizer = T5Tokenizer.from_pretrained(model_name)
tokenizer_config.json: 2.32kB [00:00, 289kB/s]
C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\Users\ASUS\.cache\huggingface\hub\models--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.
To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development
  warnings.warn(message)
spiece.model: 100%|████████████████████████████████████████████████████████████████████████| 792k/792k [00:01<00:00, 667kB/s]
tokenizer.json: 1.39MB [00:00, 1.54MB/s]
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
>>> model.save_pretrained(rf"D:\workspace_python\infinity_data\model\local_model")
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'model' is not defined
>>> model = T5ForConditionalGeneration.from_pretrained(model_name)
config.json: 1.21kB [00:00, 151kB/s]
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\modeling_utils.py", line 309, in _wrapper
    return func(*args, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\modeling_utils.py", line 4252, in from_pretrained
    device_in_context = get_torch_context_manager_or_global_device()
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\modeling_utils.py", line 322, in get_torch_context_manager_or_global_device
    default_device = torch.get_default_device()
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\torch\__init__.py", line 1833, in __getattr__
    raise AttributeError(f"module '{__name__}' has no attribute '{name}'")
AttributeError: module 'torch' has no attribute 'get_default_device'
>>> exit()

(py39_test) C:\Users\ASUS>pip install torch torchvision torchaudio --upgrade
Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple
Requirement already satisfied: torch in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (2.1.0+cpu)
Collecting torch
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/52/1b/b0cffd683414ea162ab462270ff5028b5be8e9bc6a17447960bf4d7e11c2/torch-2.7.0-cp39-cp39-win_amd64.whl (212.4 MB)
     ---------------------------------------- 212.4/212.4 MB 7.1 MB/s eta 0:00:00
Collecting torchvision
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/04/a6/9ac4d1780d7ffac2d7067e05904437c44a27ab8ca75a7b1a163d9d32bf46/torchvision-0.22.0-cp39-cp39-win_amd64.whl (1.7 MB)
     ---------------------------------------- 1.7/1.7 MB 11.6 MB/s eta 0:00:00
Collecting torchaudio
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/33/ea/cc07ef2568f582c4f3e38b68c3e76b506ead99202d4059ede930ea9c9f16/torchaudio-2.7.0-cp39-cp39-win_amd64.whl (2.5 MB)
     ---------------------------------------- 2.5/2.5 MB 8.9 MB/s eta 0:00:00
Requirement already satisfied: filelock in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from torch) (3.18.0)
Requirement already satisfied: typing-extensions>=4.10.0 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from torch) (4.13.2)
Requirement already satisfied: sympy>=1.13.3 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from torch) (1.14.0)
Requirement already satisfied: networkx in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from torch) (3.2.1)
Requirement already satisfied: jinja2 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from torch) (3.1.6)
Requirement already satisfied: fsspec in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from torch) (2025.3.0)
Requirement already satisfied: numpy in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from torchvision) (1.26.0)
Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/58/bb/87efd58b3689537a623d44dbb2550ef0bb5ff6a62769707a0fe8b1a7bdeb/pillow-11.2.1-cp39-cp39-win_amd64.whl (2.7 MB)
     ---------------------------------------- 2.7/2.7 MB 8.6 MB/s eta 0:00:00
Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from sympy>=1.13.3->torch) (1.3.0)
Requirement already satisfied: MarkupSafe>=2.0 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from jinja2->torch) (3.0.2)
Installing collected packages: pillow, torch, torchvision, torchaudio
  Attempting uninstall: torch
    Found existing installation: torch 2.1.0+cpu
    Uninstalling torch-2.1.0+cpu:
      Successfully uninstalled torch-2.1.0+cpu
Successfully installed pillow-11.2.1 torch-2.7.0 torchaudio-2.7.0 torchvision-0.22.0

(py39_test) C:\Users\ASUS>python
Python 3.9.21 (main, Dec 11 2024, 16:35:24) [MSC v.1929 64 bit (AMD64)] on win32
Type "help", "copyright", "credits" or "license" for more information.
>>> import torch
>>>
>>> # 手动定义设备
>>> device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
>>> model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'T5ForConditionalGeneration' is not defined
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration
>>>
>>> model_name = "t5-small"
>>> tokenizer = T5Tokenizer.from_pretrained(model_name)
Traceback (most recent call last):
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connection.py", line 198, in _new_conn
    sock = connection.create_connection(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    raise err
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
TimeoutError: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    response = self._make_request(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connectionpool.py", line 488, in _make_request
    raise new_e
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connectionpool.py", line 464, in _make_request
    self._validate_conn(conn)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connectionpool.py", line 1093, in _validate_conn
    conn.connect()
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connection.py", line 704, in connect
    self.sock = sock = self._new_conn()
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x000002B90B1B46D0>: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\requests\adapters.py", line 667, in send
    resp = conn.urlopen(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connectionpool.py", line 841, in urlopen
    retries = retries.increment(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\util\retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/t5-small/tree/main/additional_chat_templates?recursive=False&expand=False (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002B90B1B46D0>: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正 确答复或连接的主机没有反应，连接尝试失败。'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\tokenization_utils_base.py", line 1968, in from_pretrained
    for template in list_repo_templates(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\utils\hub.py", line 161, in list_repo_templates
    return [
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\utils\hub.py", line 161, in <listcomp>
    return [
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\hf_api.py", line 3140, in list_repo_tree
    for path_info in paginate(path=tree_url, headers=headers, params={"recursive": recursive, "expand": expand}):
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\utils\_pagination.py", line 36, in paginate
    r = session.get(path, params=params, headers=headers)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\requests\sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\requests\sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\requests\sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\utils\_http.py", line 96, in send
    return super().send(request, *args, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\requests\adapters.py", line 700, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: (MaxRetryError("HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/t5-small/tree/main/additional_chat_templates?recursive=False&expand=False (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002B90B1B46D0>: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。'))"), '(Request ID: f3fe6719-ea33-4f06-8c07-f67d3dcae121)')
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration
>>> import os
>>>
>>> os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
>>> os.environ["HUGGINGFACE_HUB_BASE_URL"] = "https://hf-mirror.com"  # 关键！覆盖内部默认 URL
>>>
>>> model_name = "t5-small"
>>> tokenizer = T5Tokenizer.from_pretrained(model_name)
Traceback (most recent call last):
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connection.py", line 198, in _new_conn
    sock = connection.create_connection(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    raise err
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
TimeoutError: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    response = self._make_request(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connectionpool.py", line 488, in _make_request
    raise new_e
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connectionpool.py", line 464, in _make_request
    self._validate_conn(conn)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connectionpool.py", line 1093, in _validate_conn
    conn.connect()
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connection.py", line 704, in connect
    self.sock = sock = self._new_conn()
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x000002B90B040D00>: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\requests\adapters.py", line 667, in send
    resp = conn.urlopen(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connectionpool.py", line 841, in urlopen
    retries = retries.increment(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\util\retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/t5-small/tree/main/additional_chat_templates?recursive=False&expand=False (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002B90B040D00>: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正 确答复或连接的主机没有反应，连接尝试失败。'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\tokenization_utils_base.py", line 1968, in from_pretrained
    for template in list_repo_templates(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\utils\hub.py", line 161, in list_repo_templates
    return [
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\utils\hub.py", line 161, in <listcomp>
    return [
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\hf_api.py", line 3140, in list_repo_tree
    for path_info in paginate(path=tree_url, headers=headers, params={"recursive": recursive, "expand": expand}):
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\utils\_pagination.py", line 36, in paginate
    r = session.get(path, params=params, headers=headers)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\requests\sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\requests\sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\requests\sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\utils\_http.py", line 96, in send
    return super().send(request, *args, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\requests\adapters.py", line 700, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: (MaxRetryError("HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/t5-small/tree/main/additional_chat_templates?recursive=False&expand=False (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002B90B040D00>: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。'))"), '(Request ID: ce5132b6-ef29-4f54-9984-bf6faef9040a)')
>>> exit()

(py39_test) C:\Users\ASUS>python
Python 3.9.21 (main, Dec 11 2024, 16:35:24) [MSC v.1929 64 bit (AMD64)] on win32
Type "help", "copyright", "credits" or "license" for more information.
>>> import os
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration
>>>
>>>
>>> os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
>>> os.environ["HUGGINGFACE_HUB_BASE_URL"] = "https://hf-mirror.com"  # 关键！覆盖内部默认 URL
>>>
>>> model_name = "t5-small"
>>> tokenizer = T5Tokenizer.from_pretrained(model_name)
Traceback (most recent call last):
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connection.py", line 198, in _new_conn
    sock = connection.create_connection(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    raise err
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
TimeoutError: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    response = self._make_request(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connectionpool.py", line 488, in _make_request
    raise new_e
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connectionpool.py", line 464, in _make_request
    self._validate_conn(conn)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connectionpool.py", line 1093, in _validate_conn
    conn.connect()
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connection.py", line 704, in connect
    self.sock = sock = self._new_conn()
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x000001BF25216820>: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\requests\adapters.py", line 667, in send
    resp = conn.urlopen(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connectionpool.py", line 841, in urlopen
    retries = retries.increment(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\util\retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/t5-small/tree/main/additional_chat_templates?recursive=False&expand=False (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001BF25216820>: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正 确答复或连接的主机没有反应，连接尝试失败。'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\tokenization_utils_base.py", line 1968, in from_pretrained
    for template in list_repo_templates(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\utils\hub.py", line 161, in list_repo_templates
    return [
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\utils\hub.py", line 161, in <listcomp>
    return [
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\hf_api.py", line 3140, in list_repo_tree
    for path_info in paginate(path=tree_url, headers=headers, params={"recursive": recursive, "expand": expand}):
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\utils\_pagination.py", line 36, in paginate
    r = session.get(path, params=params, headers=headers)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\requests\sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\requests\sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\requests\sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\utils\_http.py", line 96, in send
    return super().send(request, *args, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\requests\adapters.py", line 700, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: (MaxRetryError("HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/t5-small/tree/main/additional_chat_templates?recursive=False&expand=False (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001BF25216820>: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。'))"), '(Request ID: 700a7299-a546-464f-b034-fc10a88ca59b)')
>>> exit()

(py39_test) C:\Users\ASUS>pip install huggingface_hub
Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple
Requirement already satisfied: huggingface_hub in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (0.32.0)
Requirement already satisfied: filelock in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from huggingface_hub) (3.18.0)
Requirement already satisfied: fsspec>=2023.5.0 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from huggingface_hub) (2025.3.0)
Requirement already satisfied: packaging>=20.9 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from huggingface_hub) (25.0)
Requirement already satisfied: pyyaml>=5.1 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from huggingface_hub) (6.0.2)
Requirement already satisfied: requests in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from huggingface_hub) (2.32.3)
Requirement already satisfied: tqdm>=4.42.1 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from huggingface_hub) (4.67.1)
Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from huggingface_hub) (4.13.2)
Requirement already satisfied: colorama in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)
Requirement already satisfied: charset-normalizer<4,>=2 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from requests->huggingface_hub) (3.4.2)
Requirement already satisfied: idna<4,>=2.5 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from requests->huggingface_hub) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from requests->huggingface_hub) (2.4.0)
Requirement already satisfied: certifi>=2017.4.17 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from requests->huggingface_hub) (2025.4.26)

(py39_test) C:\Users\ASUS>huggingface-cli download --resume-download t5-small --local-dir D:\workspace_python\infinity_data\model\t5-small --local-dir-use-symlinks False
C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\commands\download.py:139: FutureWarning: Ignoring --local-dir-use-symlinks. Downloading to a local directory does not use symlinks anymore.
  warnings.warn(
Traceback (most recent call last):
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connection.py", line 198, in _new_conn
    sock = connection.create_connection(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    raise err
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
TimeoutError: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    response = self._make_request(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connectionpool.py", line 488, in _make_request
    raise new_e
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connectionpool.py", line 464, in _make_request
    self._validate_conn(conn)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connectionpool.py", line 1093, in _validate_conn
    conn.connect()
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connection.py", line 704, in connect
    self.sock = sock = self._new_conn()
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x0000016772188940>: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\requests\adapters.py", line 667, in send
    resp = conn.urlopen(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\connectionpool.py", line 841, in urlopen
    retries = retries.increment(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\urllib3\util\retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/t5-small/revision/main (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000016772188940>: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\_snapshot_download.py", line 161, in snapshot_download
    repo_info = api.repo_info(repo_id=repo_id, repo_type=repo_type, revision=revision, token=token)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\utils\_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\hf_api.py", line 2816, in repo_info
    return method(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\utils\_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\hf_api.py", line 2600, in model_info
    r = get_session().get(path, headers=headers, timeout=timeout, params=params)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\requests\sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\requests\sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\requests\sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\utils\_http.py", line 96, in send
    return super().send(request, *args, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\requests\adapters.py", line 700, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: (MaxRetryError("HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/t5-small/revision/main (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000016772188940>: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接 尝试失败。'))"), '(Request ID: 98484f73-0f09-452e-8f28-ab16567c9cb2)')

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\Scripts\huggingface-cli.exe\__main__.py", line 7, in <module>
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\commands\huggingface_cli.py", line 59, in main
    service.run()
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\commands\download.py", line 153, in run
    print(self._download())  # Print path to downloaded files
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\commands\download.py", line 187, in _download
    return snapshot_download(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\utils\_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\_snapshot_download.py", line 244, in snapshot_download
    raise LocalEntryNotFoundError(
huggingface_hub.errors.LocalEntryNotFoundError: An error happened while trying to locate the files on the Hub and we cannot find the appropriate snapshot folder for the specified revision on the local disk. Please check your internet connection and try again.

(py39_test) C:\Users\ASUS>setx HF_ENDPOINT "https://hf-mirror.com"

成功: 指定的值已得到保存。

(py39_test) C:\Users\ASUS>setx HF_HUB_ENABLE_HF_TRANSFER "1"

成功: 指定的值已得到保存。

(py39_test) C:\Users\ASUS>set HF_ENDPOINT=https://hf-mirror.com

(py39_test) C:\Users\ASUS>huggingface-cli download t5-small ^
More?   --local-dir D:\workspace_python\infinity_data\model\t5-small ^
More?   --num-workers 8
usage: huggingface-cli <command> [<args>]
huggingface-cli: error: unrecognized arguments: --num-workers 8

(py39_test) C:\Users\ASUS>huggingface-cli download t5-small ^
More?   --local-dir D:\workspace_python\infinity_data\model\t5-small ^
More?   --num-procs 8  # 正确参数名为 --num-procs
usage: huggingface-cli <command> [<args>]
huggingface-cli: error: unrecognized arguments: --num-procs 8 # 正确参数名为 --num-procs

(py39_test) C:\Users\ASUS>huggingface-cli download t5-small ^
More?   --local-dir D:\workspace_python\infinity_data\model\t5-small ^
More?   --num-procs 8
usage: huggingface-cli <command> [<args>]
huggingface-cli: error: unrecognized arguments: --num-procs 8

(py39_test) C:\Users\ASUS>huggingface-cli download t5-small ^
More?   --local-dir D:\workspace_python\infinity_data\model\t5-small ^
More?   --num-procs 8 ^
More?   --resume-download
usage: huggingface-cli <command> [<args>]
huggingface-cli: error: unrecognized arguments: --num-procs 8

(py39_test) C:\Users\ASUS>pip install --upgrade huggingface_hub>=0.26.0

(py39_test) C:\Users\ASUS>uggingface-cli download t5-small ^
More?   --local-dir D:\workspace_python\infinity_data\model\t5-small ^
More?   --num-procs 8 ^
More?   --resume-download
'uggingface-cli' 不是内部或外部命令，也不是可运行的程序
或批处理文件。

(py39_test) C:\Users\ASUS>huggingface-cli download t5-small ^
More?   --local-dir D:\workspace_python\infinity_data\model\t5-small ^
More?   --num-procs 8 ^
More?   --resume-download
usage: huggingface-cli <command> [<args>]
huggingface-cli: error: unrecognized arguments: --num-procs 8

(py39_test) C:\Users\ASUS>set HF_ENDPOINT=https://hf-mirror.com

(py39_test) C:\Users\ASUS>set HF_HUB_ENABLE_HF_TRANSFER=1  # 启用高效传输协议（网页6）

(py39_test) C:\Users\ASUS>
(py39_test) C:\Users\ASUS>huggingface-cli download t5-small ^
More?   --local-dir D:\workspace_python\infinity_data\model\t5-small ^
More?   --num-procs 8 ^
More?   --resume-download ^
More?   --local-dir-use-symlinks False
usage: huggingface-cli <command> [<args>]
huggingface-cli: error: unrecognized arguments: --num-procs 8

(py39_test) C:\Users\ASUS>python
Python 3.9.21 (main, Dec 11 2024, 16:35:24) [MSC v.1929 64 bit (AMD64)] on win32
Type "help", "copyright", "credits" or "license" for more information.
>>> from huggingface_hub import snapshot_download
>>>
>>> snapshot_download(
...     repo_id="t5-small",
...     local_dir="D:/workspace_python/infinity_data/model/t5-small",
...     num_procs=8,
...     resume_download=True,
...     local_dir_use_symlinks=False
... )
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\utils\_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
TypeError: snapshot_download() got an unexpected keyword argument 'num_procs'
>>> exit()

(py39_test) C:\Users\ASUS>huggingface-cli download t5-small ^
More?   --local-dir D:\workspace_python\infinity_data\model\t5-small
Fetching 20 files:   0%|                                                                                 | 0/20 [00:00<?, ?it/s]Still waiting to acquire lock on D:\workspace_python\infinity_data\model\t5-small\.cache\huggingface\.gitignore.lock (elapsed: 0.4 seconds)
Still waiting to acquire lock on D:\workspace_python\infinity_data\model\t5-small\.cache\huggingface\.gitignore.lock (elapsed: 0.4 seconds)
Still waiting to acquire lock on D:\workspace_python\infinity_data\model\t5-small\.cache\huggingface\.gitignore.lock (elapsed: 0.4 seconds)
Still waiting to acquire lock on D:\workspace_python\infinity_data\model\t5-small\.cache\huggingface\.gitignore.lock (elapsed: 0.4 seconds)
Downloading 'model.safetensors' to 'D:\workspace_python\infinity_data\model\t5-small\.cache\huggingface\download\xGOKKLRSlIhH692hSVvI1-gpoa8=.bd944e5f1b3ad9b70dd9d00010a517059e19265671076b8b0a4a58d9491842bc.incomplete'
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Downloading 'README.md' to 'D:\workspace_python\infinity_data\model\t5-small\.cache\huggingface\download\Xn7B-BWUGOee2Y6hCZtEhtFu4BE=.ec7fe4d8539a6aa166b27598bd402329eedc4305.incomplete'
Downloading 'generation_config.json' to 'D:\workspace_python\infinity_data\model\t5-small\.cache\huggingface\download\3EVKVggOldJcKSsGjSdoUCN1AyQ=.d52815623b46b7db1c4b957b5a83a8ad30b0146a.incomplete'
Downloading 'onnx/decoder_model.onnx' to 'D:\workspace_python\infinity_data\model\t5-small\.cache\huggingface\download\onnx\NE2Uu8jJW0gNxzFP506Cbdzq31s=.2256c6365e8243581be4d67b4cd19b9c483de8e998bf319194f4a709d0efa9a4.incomplete'
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Downloading '.gitattributes' to 'D:\workspace_python\infinity_data\model\t5-small\.cache\huggingface\download\wPaCkH-WbT7GsmxMKKrNZTV4nSM=.007c629944b66df84f5728e07d5fcab3d827c425.incomplete'
Downloading 'flax_model.msgpack' to 'D:\workspace_python\infinity_data\model\t5-small\.cache\huggingface\download\gPcsVCQDYDHk-_n0G9uADl7PXIM=.b143e13ccb7307ad36b3327ca49019f222606e945d9b995404d7200224504a9c.incomplete'
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
README.md: 8.47kB [00:00, 1.13MB/s]
generation_config.json: 100%|██████████████████████████████████████████████████████████████████| 147/147 [00:00<00:00, 48.5kB/s]
Download complete. Moving file to D:\workspace_python\infinity_data\model\t5-small\README.md          | 0.00/147 [00:00<?, ?B/s]
Download complete. Moving file to D:\workspace_python\infinity_data\model\t5-small\generation_config.json
.gitattributes: 537B [00:00, 89.2kB/s]
Download complete. Moving file to D:\workspace_python\infinity_data\model\t5-small\.gitattributes     | 0.00/121 [00:00<?, ?B/s]
Fetching 20 files:   5%|███▋                                                                     | 1/20 [00:01<00:23,  1.22s/it]Downloading 'onnx/decoder_model_merged.onnx' to 'D:\workspace_python\infinity_data\model\t5-small\.cache\huggingface\download\onnx\8nAEj8og01VMfTGLDuLvhB35wcw=.f6d2874dafd291458e59f3762915e7111cc66caaffedbffb78d2d5fa42f6cdce.incomplete'
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Downloading 'onnx/decoder_model_merged_quantized.onnx' to 'D:\workspace_python\infinity_data\model\t5-small\.cache\huggingface\download\onnx\k0Rg_u4BNd7FTuQSDoN-qFQzRr8=.af6a06ea84880ca90282e6b94e17051eea139ca9c2b8d03c45ba204c03418092.incomplete'
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Downloading 'onnx/decoder_with_past_model.onnx' to 'D:\workspace_python\infinity_data\model\t5-small\.cache\huggingface\download\onnx\LdC3-bRWeEDyMeNEeGsXefVwwVM=.2eb7790f56c47e6498e850041a4c9c60ebc8133d3792f30d91eebd242e819836.incomplete'
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Downloading 'onnx/decoder_with_past_model_quantized.onnx' to 'D:\workspace_python\infinity_data\model\t5-small\.cache\huggingface\download\onnx\SxrO2aMyqCwu_s1aIHDZ7l3dOhk=.30e4b22fc9860ee5bc4dc07c4857c6a72c81303d0a8a2d7225549127d0219caa.incomplete'
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Downloading 'onnx/decoder_model_quantized.onnx' to 'D:\workspace_python\infinity_data\model\t5-small\.cache\huggingface\download\onnx\cuKlQNPV2amgzbYaEsntLTsLeM0=.a1b7a03fd50c2c5d3bf6e0d605e70d3bf1cb0b35e99ae3bd9fd830a5b87deba3.incomplete'
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
decoder_with_past_model_quantized.onnx: 100%|██████████████████████████████████████████████| 55.2M/55.2M [00:05<00:00, 9.87MB/s]
Download complete. Moving file to D:\workspace_python\infinity_data\model\t5-small\onnx\decoder_with_past_model_quantized.onnxs]
(…)a49019f222606e945d9b995404d7200224504a9c:   9%|███▋                                      | 21.0M/242M [00:04<00:45, 4.90MB/sDownloading 'onnx/encoder_model.onnx' to 'D:\workspace_python\infinity_data\model\t5-small\.cache\huggingface\download\onnx\YeJmrBrPWXRUfSzCLt9-4E_hMZ8=.3ebb633a3a8121ca86badcd8fa84eae8a32ce49a938af351731d430f136e0dc4.incomplete'   | 0.00/233M [00:00<?, ?B/s]
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`7M [00:03<00:15, 3.12MB/s]
decoder_model_merged_quantized.onnx: 100%|█████████████████████████████████████████████████| 58.7M/58.7M [00:17<00:00, 3.36MB/s]
Download complete. Moving file to D:\workspace_python\infinity_data\model\t5-small\onnx\decoder_model_merged_quantized.onnxMB/s]
(…)a49019f222606e945d9b995404d7200224504a9c:  43%|██████████████████▋                        | 105M/242M [00:17<00:27, 4.94MB/sDownloading 'onnx/encoder_model_quantized.onnx' to 'D:\workspace_python\infinity_data\model\t5-small\.cache\huggingface\download\onnx\wgmU71oqVonjPd3A3HT3Yk5Lx3k=.5124770e67bf2c2ca80dd49ff9a002b240c3adbaf0c3b96a765115a756ed2916.incomplete':18<00:18, 6.93MB/s]
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`7M [00:17<00:00, 3.39MB/s]
(…)f9a002b240c3adbaf0c3b96a765115a756ed2916: 100%|█████████████████████████████████████████| 35.5M/35.5M [00:05<00:00, 6.37MB/s]
Download complete. Moving file to D:\workspace_python\infinity_data\model\t5-small\onnx\encoder_model_quantized.onnx0, 1.40MB/s]
(…)a49019f222606e945d9b995404d7200224504a9c:  65%|███████████████████████████▉               | 157M/242M [00:23<00:10, 7.71MB/sDownloading 'pytorch_model.bin' to 'D:\workspace_python\infinity_data\model\t5-small\.cache\huggingface\download\Q1p2l2BzM1m6P5jKvr8WTq1TUio=.dd8c1c79a54cce728b86bac6c7737800fc1ec009ada37028efff4861a4280ffc.incomplete'     | 31.5M/233M [00:20<02:10, 1.54MB/s]
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`33M [00:24<01:46, 1.79MB/s]
(…)a49019f222606e945d9b995404d7200224504a9c: 100%|███████████████████████████████████████████| 242M/242M [00:38<00:00, 6.23MB/s]
Download complete. Moving file to D:\workspace_python\infinity_data\model\t5-small\flax_model.msgpack42M [00:37<02:05, 1.51MB/s]
Fetching 20 files:  20%|██████████████▌                                                          | 4/20 [00:40<02:55, 10.95s/it]Downloading 'rust_model.ot' to 'D:\workspace_python\infinity_data\model\t5-small\.cache\huggingface\download\ee-WZw9U-e9t7gTGo9Yn82pHA_c=.636b17628823af7a43631a408e0253a5fe9ae39ea02a87ec8d3371ea613e5b16.incomplete'         | 62.9M/233M [00:36<01:36, 1.76MB/s]
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`2M [00:12<00:38, 4.62MB/s]
encoder_model.onnx: 100%|████████████████████████████████████████████████████████████████████| 141M/141M [00:46<00:00, 3.05MB/s]
Download complete. Moving file to D:\workspace_python\infinity_data\model\t5-small\onnx\encoder_model.onnx00:51<01:54, 1.48MB/s]
Downloading 'tf_model.h5' to 'D:\workspace_python\infinity_data\model\t5-small\.cache\huggingface\download\a7eHxRFT3OeMBIFg52k2nfj5m7w=.fc9d2ec530ef7139ced7d6164339abf00c2d751f99a73c1c4b127be5d8a7c7d9.incomplete'          | 52.4M/232M [00:52<02:41, 1.12MB/s]
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
pytorch_model.bin: 100%|█████████████████████████████████████████████████████████████████████| 242M/242M [00:55<00:00, 4.38MB/s]
Download complete. Moving file to D:\workspace_python\infinity_data\model\t5-small\pytorch_model.bin242M [01:15<01:42, 1.34MB/s]
rust_model.ot: 100%|█████████████████████████████████████████████████████████████████████████| 242M/242M [00:50<00:00, 4.79MB/s]
Download complete. Moving file to D:\workspace_python\infinity_data\model\t5-small\rust_model.ot26M/242M [01:28<01:16, 1.51MB/s]]
(…)4339abf00c2d751f99a73c1c4b127be5d8a7c7d9: 100%|███████████████████████████████████████████| 242M/242M [00:44<00:00, 5.47MB/s]
Download complete. Moving file to D:\workspace_python\infinity_data\model\t5-small\tf_model.h5 136M/242M [01:34<01:07, 1.58MB/s]]
(…)2915e7111cc66caaffedbffb78d2d5fa42f6cdce: 100%|███████████████████████████████████████████| 233M/233M [01:48<00:00, 2.15MB/s]
Download complete. Moving file to D:\workspace_python\infinity_data\model\t5-small\onnx\decoder_model_merged.onnx0:35, 2.08MB/s]]
(…)4cd19b9c483de8e998bf319194f4a709d0efa9a4: 100%|███████████████████████████████████████████| 232M/232M [01:57<00:00, 1.97MB/s]
Download complete. Moving file to D:\workspace_python\infinity_data\model\t5-small\onnx\decoder_model.onnx01:57<00:09, 3.53MB/s]]
model.safetensors: 100%|█████████████████████████████████████████████████████████████████████| 242M/242M [02:04<00:00, 1.94MB/s]]
Download complete. Moving file to D:\workspace_python\infinity_data\model\t5-small\model.safetensors242M [02:04<00:00, 4.10MB/s]]
decoder_model_quantized.onnx: 100%|█████████████████████████████████████████████████████████| 58.4M/58.4M [02:04<00:00, 471kB/s]
Download complete. Moving file to D:\workspace_python\infinity_data\model\t5-small\onnx\decoder_model_quantized.onnx05, 194kB/s]
decoder_with_past_model.onnx: 100%|██████████████████████████████████████████████████████████| 220M/220M [03:39<00:00, 1.00MB/s]
Download complete. Moving file to D:\workspace_python\infinity_data\model\t5-small\onnx\decoder_with_past_model.onnx
Fetching 20 files: 100%|████████████████████████████████████████████████████████████████████████| 20/20 [03:41<00:00, 11.07s/it]
D:\workspace_python\infinity_data\model\t5-small
decoder_model_quantized.onnx:  90%|███████████████████████████████████████████████████▏     | 52.4M/58.4M [02:01<00:11, 537kB/s]
(py39_test) C:\Users\ASUS>nx: 100%|██████████████████████████████████████████████████████████| 220M/220M [03:39<00:00, 2.12MB/s]
(py39_test) C:\Users\ASUS>nx: 100%|█████████████████████████████████████████████████████████| 58.4M/58.4M [02:03<00:00, 647kB/s]
(py39_test) C:\Users\ASUS>
(py39_test) C:\Users\ASUS>
(py39_test) C:\Users\ASUS>huggingface-cli download t5-small ^
(py39_test) C:\Users\ASUS>python
Python 3.9.21 (main, Dec 11 2024, 16:35:24) [MSC v.1929 64 bit (AMD64)] on win32
Type "help", "copyright", "credits" or "license" for more information.
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration
>>>
>>> model_name = "t5-small"
>>> tokenizer = T5Tokenizer.from_pretrained(model_name)
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
>>> model = T5ForConditionalGeneration.from_pretrained(model_name)
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
model.safetensors: 100%|█████████████████████████████████████████████████████████████████████| 242M/242M [00:16<00:00, 14.7MB/s]
C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\Users\ASUS\.cache\huggingface\hub\models--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.
To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development
  warnings.warn(message)
generation_config.json: 100%|██████████████████████████████████████████████████████████████████| 147/147 [00:00<00:00, 49.2kB/s]
>>> input_text = "translate English to SQL: What is the name of the first employee?"
>>> inputs = tokenizer(input_text, return_tensors="pt")  # "pt" 表示 PyTorch 张量
>>> outputs = model.generate(**inputs, max_length=128)
>>> print(tokenizer.decode(outputs[0], skip_special_tokens=True))
Was ist der Name des ersten Mitarbeiters?
>>> def preprocess_function(examples):
...     inputs = [f"translate English to SQL: {q}" for q in examples["question"]]
...     targets = examples["sql"]["human_readable"]
...     model_inputs = tokenizer(inputs, max_length=128, truncation=True)
...
>>>     # 标记目标文本
>>>     labels = tokenizer(targets, max_length=128, truncation=True)
  File "<stdin>", line 1
    labels = tokenizer(targets, max_length=128, truncation=True)
IndentationError: unexpected indent
>>>     model_inputs["labels"] = labels["input_ids"]
  File "<stdin>", line 1
    model_inputs["labels"] = labels["input_ids"]
IndentationError: unexpected indent
>>>     return model_inputs
  File "<stdin>", line 1
    return model_inputs
IndentationError: unexpected indent
>>>
>>> tokenized_dataset = dataset.map(preprocess_function, batched=True)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'dataset' is not defined
>>>
>>>
>>> def preprocess_function(examples):
...     inputs = [f"translate English to SQL: {q}" for q in examples["question"]]
...     targets = examples["sql"]["human_readable"]
...     model_inputs = tokenizer(inputs, max_length=128, truncation=True)
...     # 标记目标文本
...     labels = tokenizer(targets, max_length=128, truncation=True)
...     model_inputs["labels"] = labels["input_ids"]
...     return model_inputs
...
>>> tokenized_dataset = dataset.map(preprocess_function, batched=True)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'dataset' is not defined
>>> from datasets import load_dataset
>>>
>>> # 加载WikiSQL数据集（较小，适合演示）
>>> dataset = load_dataset("wikisql")
README.md: 7.80kB [00:00, ?B/s]
C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\Users\ASUS\.cache\huggingface\hub\datasets--wikisql. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.
To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development
  warnings.warn(message)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 1782, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 1664, in dataset_module_factory
    raise e1 from None
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 1607, in dataset_module_factory
    return HubDatasetModuleFactoryWithScript(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 1264, in get_module
    trust_remote_code = resolve_trust_remote_code(self.trust_remote_code, self.name)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 137, in resolve_trust_remote_code
    raise ValueError(
ValueError: The repository for wikisql contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wikisql.
Please pass the argument `trust_remote_code=True` to allow custom code to be run.
>>> print(dataset["train"][0])  # 查看数据结构
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'dataset' is not defined
>>>
>>> from datasets import load_dataset
>>>
>>> # 加载WikiSQL数据集（较小，适合演示）
>>> dataset = load_dataset("wikisql", trust_remote_code=True)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 1819, in load_dataset_builder
    builder_instance: DatasetBuilder = builder_cls(
TypeError: 'NoneType' object is not callable
>>> print(dataset["train"][0])  # 查看数据结构
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'dataset' is not defined
>>>
>>>
>>>
>>> from huggingface_hub import delete_cache
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
ImportError: cannot import name 'delete_cache' from 'huggingface_hub' (C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\__init__.py)
>>> delete_cache(repo_id="wikisql")  # 清理指定数据集缓存
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'delete_cache' is not defined
>>>
>>> # 强制重新下载并加载
>>> dataset = load_dataset("wikisql", trust_remote_code=True, download_mode="force_redownload")
README.md: 7.80kB [00:00, ?B/s]
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 1819, in load_dataset_builder
    builder_instance: DatasetBuilder = builder_cls(
TypeError: 'NoneType' object is not callable
>>> from huggingface_hub import delete_cache
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
ImportError: cannot import name 'delete_cache' from 'huggingface_hub' (C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\__init__.py)
>>>
>>>
>>>
>>> import os
>>> os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
>>> from datasets import load_dataset
>>>
>>> # 加载WikiSQL数据集（较小，适合演示）
>>> dataset = load_dataset("wikisql", trust_remote_code=True)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 1819, in load_dataset_builder
    builder_instance: DatasetBuilder = builder_cls(
TypeError: 'NoneType' object is not callable
>>> from huggingface_hub import delete_cache
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
ImportError: cannot import name 'delete_cache' from 'huggingface_hub' (C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\__init__.py)
>>> from huggingface_hub import __version__
>>> print(__version__)  # 确认版本 ≥0.26.0
0.32.0
>>> import shutil
>>> cache_path = "C:/Users/ASUS/.cache/huggingface/hub"
>>> shutil.rmtree(cache_path, ignore_errors=True)
>>> exit()

(py39_test) C:\Users\ASUS>pip install huggingface_hub["cli"]
Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple
Requirement already satisfied: huggingface_hub[cli] in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (0.32.0)
Requirement already satisfied: filelock in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from huggingface_hub[cli]) (3.18.0)
Requirement already satisfied: fsspec>=2023.5.0 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from huggingface_hub[cli]) (2025.3.0)
Requirement already satisfied: packaging>=20.9 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from huggingface_hub[cli]) (25.0)
Requirement already satisfied: pyyaml>=5.1 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from huggingface_hub[cli]) (6.0.2)
Requirement already satisfied: requests in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from huggingface_hub[cli]) (2.32.3)
Requirement already satisfied: tqdm>=4.42.1 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from huggingface_hub[cli]) (4.67.1)
Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from huggingface_hub[cli]) (4.13.2)
Collecting InquirerPy==0.3.4 (from huggingface_hub[cli])
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ce/ff/3b59672c47c6284e8005b42e84ceba13864aa0f39f067c973d1af02f5d91/InquirerPy-0.3.4-py3-none-any.whl (67 kB)
Collecting pfzy<0.4.0,>=0.3.1 (from InquirerPy==0.3.4->huggingface_hub[cli])
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/8c/d7/8ff98376b1acc4503253b685ea09981697385ce344d4e3935c2af49e044d/pfzy-0.3.4-py3-none-any.whl (8.5 kB)
Collecting prompt-toolkit<4.0.0,>=3.0.1 (from InquirerPy==0.3.4->huggingface_hub[cli])
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ce/4f/5249960887b1fbe561d9ff265496d170b55a735b76724f10ef19f9e40716/prompt_toolkit-3.0.51-py3-none-any.whl (387 kB)
Collecting wcwidth (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface_hub[cli])
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/fd/84/fd2ba7aafacbad3c4201d395674fc6348826569da3c0937e75505ead3528/wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)
Requirement already satisfied: colorama in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from tqdm>=4.42.1->huggingface_hub[cli]) (0.4.6)
Requirement already satisfied: charset-normalizer<4,>=2 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from requests->huggingface_hub[cli]) (3.4.2)
Requirement already satisfied: idna<4,>=2.5 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from requests->huggingface_hub[cli]) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from requests->huggingface_hub[cli]) (2.4.0)
Requirement already satisfied: certifi>=2017.4.17 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from requests->huggingface_hub[cli]) (2025.4.26)
Installing collected packages: wcwidth, prompt-toolkit, pfzy, InquirerPy
Successfully installed InquirerPy-0.3.4 pfzy-0.3.4 prompt-toolkit-3.0.51 wcwidth-0.2.13

(py39_test) C:\Users\ASUS>huggingface-cli delete-cache
?? Select revisions to delete: 0 revision(s) selected.
Deletion is cancelled. Do nothing.

(py39_test) C:\Users\ASUS>python
Python 3.9.21 (main, Dec 11 2024, 16:35:24) [MSC v.1929 64 bit (AMD64)] on win32
Type "help", "copyright", "credits" or "license" for more information.
>>> from datasets import load_dataset
>>> dataset = load_dataset("wikisql")
README.md: 7.80kB [00:00, ?B/s]
C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\Users\ASUS\.cache\huggingface\hub\datasets--wikisql. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.
To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development
  warnings.warn(message)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 1819, in load_dataset_builder
    builder_instance: DatasetBuilder = builder_cls(
TypeError: 'NoneType' object is not callable
>>> print(dataset["train"][0])  # 查看数据结构
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'dataset' is not defined
>>> exit()

(py39_test) C:\Users\ASUS>pip install --upgrade datasets>=2.16.0 huggingface_hub>=0.26.0

(py39_test) C:\Users\ASUS>python
Python 3.9.21 (main, Dec 11 2024, 16:35:24) [MSC v.1929 64 bit (AMD64)] on win32
Type "help", "copyright", "credits" or "license" for more information.
>>> import shutil
>>> shutil.rmtree(r"C:\Users\ASUS\.cache\huggingface\datasets", ignore_errors=True)
>>> import os
>>> os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
>>>
>>> dataset = load_dataset(
...     "wikisql",
...     trust_remote_code=True,  # 允许执行远程代码[9](@ref)
...     download_mode="force_redownload",
...     cache_dir="D:\workspace_python\infinity_data\model\cache\wikisql"  # 自定义缓存路径（避免C盘权限问题）
... )
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'load_dataset' is not defined
>>>
>>> from datasets import load_dataset
>>> import os
>>> os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
>>>
>>> dataset = load_dataset(
...     "wikisql",
...     trust_remote_code=True,  # 允许执行远程代码[9](@ref)
...     download_mode="force_redownload",
...     cache_dir="D:\workspace_python\infinity_data\model\cache\wikisql"  # 自定义缓存路径（避免C盘权限问题）
... )
README.md: 7.80kB [00:00, ?B/s]
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 1819, in load_dataset_builder
    builder_instance: DatasetBuilder = builder_cls(
TypeError: 'NoneType' object is not callable
>>> exit
Use exit() or Ctrl-Z plus Return to exit
>>> exit()

(py39_test) C:\Users\ASUS>pip install --upgrade datasets>=2.16.0 huggingface_hub>=0.26.0

(py39_test) C:\Users\ASUS>pip install fsspec==2023.12.0
Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple
Collecting fsspec==2023.12.0
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/67/32/9276db0647d8142da3d9ec1af536522081813005a9d7aaebbdba082967c1/fsspec-2023.12.0-py3-none-any.whl (168 kB)
Installing collected packages: fsspec
  Attempting uninstall: fsspec
    Found existing installation: fsspec 2025.3.0
    Uninstalling fsspec-2025.3.0:
      Successfully uninstalled fsspec-2025.3.0
Successfully installed fsspec-2023.12.0

(py39_test) C:\Users\ASUS>import shutil
'import' 不是内部或外部命令，也不是可运行的程序
或批处理文件。

(py39_test) C:\Users\ASUS>cache_path = r"D:\workspace_python\infinity_data\model\cache\wikisql"
'cache_path' 不是内部或外部命令，也不是可运行的程序
或批处理文件。

(py39_test) C:\Users\ASUS>shutil.rmtree(cache_path, ignore_errors=True)

(py39_test) C:\Users\ASUS>python
Python 3.9.21 (main, Dec 11 2024, 16:35:24) [MSC v.1929 64 bit (AMD64)] on win32
Type "help", "copyright", "credits" or "license" for more information.
>>> import shutil
>>> cache_path = r"D:\workspace_python\infinity_data\model\cache\wikisql"
>>> shutil.rmtree(cache_path, ignore_errors=True)
>>> from datasets import load_dataset
>>> import os
>>>
>>> # 配置镜像站和高效传输协议
>>> os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
>>> os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "1"
>>>
>>> # 加载数据集（新增 max_retries 和 num_proc）
>>> dataset = load_dataset(
...     "wikisql",
...     trust_remote_code=True,
...     download_mode="force_redownload",
...     cache_dir="D:/workspace_python/infinity_data/model/cache/wikisql",
...     max_retries=5,  # 增加重试次数
...     num_proc=4      # 多进程加速
... )
README.md: 7.80kB [00:00, 7.83MB/s]
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 1819, in load_dataset_builder
    builder_instance: DatasetBuilder = builder_cls(
TypeError: 'NoneType' object is not callable
>>> exit()

(py39_test) C:\Users\ASUS>huggingface-cli download Salesforce/wikisql ^
More?     --local-dir D:\workspace_python\infinity_data\model\wikisql ^
More?     --files train.jsonl,dev.jsonl,test.jsonl ^
More?     --local-dir-use-symlinks False
usage: huggingface-cli <command> [<args>]
huggingface-cli: error: unrecognized arguments: --files train.jsonl,dev.jsonl,test.jsonl

(py39_test) C:\Users\ASUS>huggingface-cli download Salesforce/wikisql --local-dir D:\workspace_python\infinity_data\model\wikisql
Traceback (most recent call last):
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\utils\_http.py", line 409, in hf_raise_for_status
    response.raise_for_status()
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\requests\models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://hf-mirror.com/api/models/Salesforce/wikisql/revision/main

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\Scripts\huggingface-cli.exe\__main__.py", line 7, in <module>
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\commands\huggingface_cli.py", line 59, in main
    service.run()
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\commands\download.py", line 153, in run
    print(self._download())  # Print path to downloaded files
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\commands\download.py", line 187, in _download
    return snapshot_download(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\utils\_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\_snapshot_download.py", line 241, in snapshot_download
    raise api_call_error
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\_snapshot_download.py", line 161, in snapshot_download
    repo_info = api.repo_info(repo_id=repo_id, repo_type=repo_type, revision=revision, token=token)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\utils\_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\hf_api.py", line 2816, in repo_info
    return method(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\utils\_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\hf_api.py", line 2601, in model_info
    hf_raise_for_status(r)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\utils\_http.py", line 459, in hf_raise_for_status
    raise _format(RepositoryNotFoundError, message, response) from e
huggingface_hub.errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-6831c320-7eb799b85caeb81976845dd3;6baa41e2-c96a-4604-af47-dba4f44c67f7)

Repository Not Found for url: https://hf-mirror.com/api/models/Salesforce/wikisql/revision/main.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication
Invalid username or password.

(py39_test) C:\Users\ASUS>curl -I https://hf-mirror.com/datasets/Salesforce/wikisql
HTTP/1.1 200 OK
Alt-Svc: h3=":443"; ma=2592000
Content-Type: text/html; charset=utf-8
Cross-Origin-Opener-Policy: same-origin
Date: Sat, 24 May 2025 13:01:52 GMT
Etag: W/"22ada-QM/7HVWknn5zEd6vgJLwnvL9am0"
Referrer-Policy: strict-origin-when-cross-origin
Server: hf-mirror
Via: 1.1 3b2eef144c7f8e5b4642542316d6f5f8.cloudfront.net (CloudFront)
X-Amz-Cf-Id: p6lRDrIH2awHmEvo4WHX_zpZ_0ARIxlhsrxhSb9dSSMdEaECaDvFSg==
X-Amz-Cf-Pop: NRT12-P2
X-Cache: Miss from cloudfront
X-Frame-Options: DENY
X-Powered-By: huggingface-moon
X-Request-Id: Root=1-6831c340-4ec54ae32907cb1443db78fe


(py39_test) C:\Users\ASUS>python
Python 3.9.21 (main, Dec 11 2024, 16:35:24) [MSC v.1929 64 bit (AMD64)] on win32
Type "help", "copyright", "credits" or "license" for more information.
>>> import os
>>> os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
>>>
>>> from datasets import load_dataset
>>>
>>> # 指定版本（如 "refs/convert/parquet" 是官方维护的稳定分支）
>>> dataset = load_dataset(
...     "Salesforce/wikisql",
...     trust_remote_code=True,
...     revision="refs/convert/parquet"
... )
0000.parquet: 100%|█████████████████████████████████████████████████████████████████████████| 25.2M/25.2M [00:45<00:00, 553kB/s]
C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\Users\ASUS\.cache\huggingface\hub\datasets--Salesforce--wikisql. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.
To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development
  warnings.warn(message)
0000.parquet: 100%|████████████████████████████████████████████████████████████████████████| 3.63M/3.63M [00:02<00:00, 1.78MB/s]
0000.parquet: 100%|████████████████████████████████████████████████████████████████████████| 7.71M/7.71M [00:03<00:00, 2.43MB/s]
Generating train split: 56355 examples [00:00, 98635.82 examples/s]
Generating validation split: 8421 examples [00:00, 115605.05 examples/s]
Generating test split: 15878 examples [00:00, 108610.36 examples/s]
>>>
>>> print(dataset["train"][0])  # 查看数据结构
{'phase': 1, 'question': 'Tell me what the notes are for South Australia ', 'table': {'header': ['State/territory', 'Text/background colour', 'Format', 'Current slogan', 'Current series', 'Notes'], 'page_title': '', 'page_id': '', 'types': ['text', 'text', 'text', 'text', 'text', 'text'], 'id': '1-1000181-1', 'section_title': '', 'caption': '', 'rows': [['Australian Capital Territory', 'blue/white', 'Yaa·nna', 'ACT · CELEBRATION OF A CENTURY 2013', 'YIL·00A', 'Slogan screenprinted on plate'], ['New South Wales', 'black/yellow', 'aa·nn·aa', 'NEW SOUTH WALES', 'BX·99·HI', 'No slogan on current series'], ['New South Wales', 'black/white', 'aaa·nna', 'NSW', 'CPX·12A', 'Optional white slimline series'], ['Northern Territory', 'ochre/white', 'Ca·nn·aa', 'NT  · OUTBACK AUSTRALIA', 'CB·06·ZZ', 'New series began in June 2011'], ['Queensland', 'maroon/white', 'nnn·aaa', 'QUEENSLAND · SUNSHINE STATE', '999·TLG', 'Slogan embossed on plate'], ['South Australia', 'black/white', 'Snnn·aaa', 'SOUTH AUSTRALIA', 'S000·AZD', 'No slogan on current series'], ['Victoria', 'blue/white', 'aaa·nnn', 'VICTORIA - THE PLACE TO BE', 'ZZZ·562', 'Current series will be exhausted this year']], 'name': 'table_1000181_1'}, 'sql': {'human_readable': 'SELECT Notes FROM table WHERE Current slogan = SOUTH AUSTRALIA', 'sel': 5, 'agg': 0, 'conds': {'column_index': [3], 'operator_index': [0], 'condition': ['SOUTH AUSTRALIA']}}}
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration
>>>
>>> model_name = "t5-small"
>>> tokenizer = T5Tokenizer.from_pretrained(model_name)
tokenizer_config.json: 2.32kB [00:00, 587kB/s]
C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\huggingface_hub\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\Users\ASUS\.cache\huggingface\hub\models--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.
To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development
  warnings.warn(message)
spiece.model: 100%|███████████████████████████████████████████████████████████████████████████| 792k/792k [00:01<00:00, 610kB/s]
tokenizer.json: 1.39MB [00:00, 2.06MB/s]
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
>>> model = T5ForConditionalGeneration.from_pretrained(model_name)
config.json: 1.21kB [00:00, 605kB/s]
generation_config.json: 100%|██████████████████████████████████████████████████████████████████| 147/147 [00:00<00:00, 48.7kB/s]
>>> def preprocess_function(examples):
...     inputs = [f"translate English to SQL: {q}" for q in examples["question"]]
...     targets = examples["sql"]["human_readable"]
...     model_inputs = tokenizer(inputs, max_length=128, truncation=True)
...     # 标记目标文本
...     labels = tokenizer(targets, max_length=128, truncation=True)
...     model_inputs["labels"] = labels["input_ids"]
...     return model_inputs
...
>>> tokenized_dataset = dataset.map(preprocess_function, batched=True)
Map:   0%|                                                                                     | 0/56355 [00:00<?, ? examples/s]
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\dataset_dict.py", line 944, in map
    dataset_dict[split] = dataset.map(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\arrow_dataset.py", line 557, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\arrow_dataset.py", line 3079, in map
    for rank, done, content in Dataset._map_single(**dataset_kwargs):
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\arrow_dataset.py", line 3525, in _map_single
    for i, batch in iter_outputs(shard_iterable):
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\arrow_dataset.py", line 3475, in iter_outputs
    yield i, apply_function(example, i, offset=offset)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\arrow_dataset.py", line 3398, in apply_function
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "<stdin>", line 3, in preprocess_function
TypeError: list indices must be integers or slices, not str
>>> def preprocess_function(examples):
...     inputs = [f"translate English to SQL: {q}" for q in examples["question"]]
...
>>>     # 手动构造 SQL 语句（从 sel/agg/conds 字段提取信息）
>>>     targets = []
  File "<stdin>", line 1
    targets = []
IndentationError: unexpected indent
>>>     for i in range(len(examples["sql"])):
  File "<stdin>", line 1
    for i in range(len(examples["sql"])):
IndentationError: unexpected indent
>>>         sql_info = examples["sql"][i]
  File "<stdin>", line 1
    sql_info = examples["sql"][i]
IndentationError: unexpected indent
>>>         # 提取 SELECT 列名（需从 table 的 header 中映射）
>>>         selected_col = examples["table"][i]["header"][sql_info["sel"]]
  File "<stdin>", line 1
    selected_col = examples["table"][i]["header"][sql_info["sel"]]
IndentationError: unexpected indent
>>>         # 提取 WHERE 条件（需处理多个条件）
>>>         conditions = []
  File "<stdin>", line 1
    conditions = []
IndentationError: unexpected indent
>>>         for col_idx, op_idx, value in zip(sql_info["conds"]["column_index"],
  File "<stdin>", line 1
    for col_idx, op_idx, value in zip(sql_info["conds"]["column_index"],
IndentationError: unexpected indent
>>>                                           sql_info["conds"]["operator_index"],
  File "<stdin>", line 1
    sql_info["conds"]["operator_index"],
IndentationError: unexpected indent
>>>                                           sql_info["conds"]["condition"]):
  File "<stdin>", line 1
    sql_info["conds"]["condition"]):
IndentationError: unexpected indent
>>>             col_name = examples["table"][i]["header"][col_idx]
  File "<stdin>", line 1
    col_name = examples["table"][i]["header"][col_idx]
IndentationError: unexpected indent
>>>             op = ["=", ">", "<", "LIKE"][op_idx]  # 根据 WikiSQL 的 operator 定义调整
  File "<stdin>", line 1
    op = ["=", ">", "<", "LIKE"][op_idx]  # 根据 WikiSQL 的 operator 定义调整
IndentationError: unexpected indent
>>>             conditions.append(f"{col_name} {op} {repr(value)}")
  File "<stdin>", line 1
    conditions.append(f"{col_name} {op} {repr(value)}")
IndentationError: unexpected indent
>>>         where_clause = " AND ".join(conditions) if conditions else ""
  File "<stdin>", line 1
    where_clause = " AND ".join(conditions) if conditions else ""
IndentationError: unexpected indent
>>>
>>>         # 构造完整 SQL
>>>         target_sql = f"SELECT {selected_col} FROM table"
  File "<stdin>", line 1
    target_sql = f"SELECT {selected_col} FROM table"
IndentationError: unexpected indent
>>>         if where_clause:
  File "<stdin>", line 1
    if where_clause:
IndentationError: unexpected indent
>>>             target_sql += f" WHERE {where_clause}"
  File "<stdin>", line 1
    target_sql += f" WHERE {where_clause}"
IndentationError: unexpected indent
>>>         targets.append(target_sql)
  File "<stdin>", line 1
    targets.append(target_sql)
IndentationError: unexpected indent
>>>
>>>     # Tokenize 输入和输出
>>>     model_inputs = tokenizer(inputs, max_length=128, truncation=True)
  File "<stdin>", line 1
    model_inputs = tokenizer(inputs, max_length=128, truncation=True)
IndentationError: unexpected indent
>>>     labels = tokenizer(targets, max_length=128, truncation=True)
  File "<stdin>", line 1
    labels = tokenizer(targets, max_length=128, truncation=True)
IndentationError: unexpected indent
>>>     model_inputs["labels"] = labels["input_ids"]
  File "<stdin>", line 1
    model_inputs["labels"] = labels["input_ids"]
IndentationError: unexpected indent
>>>     return model_inputs
  File "<stdin>", line 1
    return model_inputs
IndentationError: unexpected indent
>>>
>>> def preprocess_function(examples):
...     inputs = [f"translate English to SQL: {q}" for q in examples["question"]]
...     # 手动构造 SQL 语句（从 sel/agg/conds 字段提取信息）
...     targets = []
...     for i in range(len(examples["sql"])):
...         sql_info = examples["sql"][i]
...         # 提取 SELECT 列名（需从 table 的 header 中映射）
...         selected_col = examples["table"][i]["header"][sql_info["sel"]]
...         # 提取 WHERE 条件（需处理多个条件）
...         conditions = []
...         for col_idx, op_idx, value in zip(sql_info["conds"]["column_index"],
...                                           sql_info["conds"]["operator_index"],
...                                           sql_info["conds"]["condition"]):
...             col_name = examples["table"][i]["header"][col_idx]
...             op = ["=", ">", "<", "LIKE"][op_idx]  # 根据 WikiSQL 的 operator 定义调整
...             conditions.append(f"{col_name} {op} {repr(value)}")
...         where_clause = " AND ".join(conditions) if conditions else ""
...         # 构造完整 SQL
...         target_sql = f"SELECT {selected_col} FROM table"
...         if where_clause:
...             target_sql += f" WHERE {where_clause}"
...         targets.append(target_sql)
...     # Tokenize 输入和输出
...     model_inputs = tokenizer(inputs, max_length=128, truncation=True)
...     labels = tokenizer(targets, max_length=128, truncation=True)
...     model_inputs["labels"] = labels["input_ids"]
...     return model_inputs
...
>>> print(dataset["train"].features)  # 查看所有字段类型
{'phase': Value(dtype='int32', id=None), 'question': Value(dtype='string', id=None), 'table': {'header': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'page_title': Value(dtype='string', id=None), 'page_id': Value(dtype='string', id=None), 'types': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'id': Value(dtype='string', id=None), 'section_title': Value(dtype='string', id=None), 'caption': Value(dtype='string', id=None), 'rows': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None), 'name': Value(dtype='string', id=None)}, 'sql': {'human_readable': Value(dtype='string', id=None), 'sel': Value(dtype='int32', id=None), 'agg': Value(dtype='int32', id=None), 'conds': Sequence(feature={'column_index': Value(dtype='int32', id=None), 'operator_index': Value(dtype='int32', id=None), 'condition': Value(dtype='string', id=None)}, length=-1, id=None)}}
>>> from transformers import TrainingArguments, Trainer
>>>
>>> training_args = TrainingArguments(
...     output_dir="D:\workspace_python\infinity_data\model\t5-small-result",
...     learning_rate=2e-5,
...     per_device_train_batch_size=8,
...     num_train_epochs=3,
...     weight_decay=0.01,
...     logging_dir="./logs",
... )
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "<string>", line 131, in __init__
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\training_args.py", line 1738, in __post_init__
    self.device
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\training_args.py", line 2268, in device
    return self._setup_devices
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\utils\generic.py", line 67, in __get__
    cached = self.fget(obj)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\training_args.py", line 2138, in _setup_devices
    raise ImportError(
ImportError: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`
>>> exit()

(py39_test) C:\Users\ASUS>pip install --upgrade accelerate -i https://pypi.tuna.tsinghua.edu.cn/simple
Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple
Collecting accelerate
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/f8/bb/be8146c196ad6e4dec78385d91e92591f8a433576c4e04c342a636fcd811/accelerate-1.7.0-py3-none-any.whl (362 kB)
Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from accelerate) (1.26.0)
Requirement already satisfied: packaging>=20.0 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from accelerate) (25.0)
Collecting psutil (from accelerate)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/50/1b/6921afe68c74868b4c9fa424dad3be35b095e16687989ebbb50ce4fceb7c/psutil-7.0.0-cp37-abi3-win_amd64.whl (244 kB)
Requirement already satisfied: pyyaml in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from accelerate) (6.0.2)
Requirement already satisfied: torch>=2.0.0 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from accelerate) (2.7.0)
Requirement already satisfied: huggingface-hub>=0.21.0 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from accelerate) (0.32.0)
Requirement already satisfied: safetensors>=0.4.3 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from accelerate) (0.5.3)
Requirement already satisfied: filelock in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)
Requirement already satisfied: fsspec>=2023.5.0 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from huggingface-hub>=0.21.0->accelerate) (2023.12.0)
Requirement already satisfied: requests in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)
Requirement already satisfied: tqdm>=4.42.1 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)
Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.2)
Requirement already satisfied: sympy>=1.13.3 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from torch>=2.0.0->accelerate) (1.14.0)
Requirement already satisfied: networkx in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from torch>=2.0.0->accelerate) (3.2.1)
Requirement already satisfied: jinja2 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from torch>=2.0.0->accelerate) (3.1.6)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)
Requirement already satisfied: colorama in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.6)
Requirement already satisfied: MarkupSafe>=2.0 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)
Requirement already satisfied: charset-normalizer<4,>=2 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.2)
Requirement already satisfied: idna<4,>=2.5 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)
Requirement already satisfied: certifi>=2017.4.17 in c:\users\asus\anaconda3\envs\py39_test\lib\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.4.26)
Installing collected packages: psutil, accelerate
Successfully installed accelerate-1.7.0 psutil-7.0.0

(py39_test) C:\Users\ASUS>python
Python 3.9.21 (main, Dec 11 2024, 16:35:24) [MSC v.1929 64 bit (AMD64)] on win32
Type "help", "copyright", "credits" or "license" for more information.
>>> import accelerate
>>> print(accelerate.__version__)  # 应输出 >=0.26.0
1.7.0
>>> from transformers import TrainingArguments, Trainer  # 重新导入，无报错则成功
>>> from transformers import TrainingArguments, Trainer
>>>
>>> training_args = TrainingArguments(
...     output_dir="D:\workspace_python\infinity_data\model\t5-small-result",
...     learning_rate=2e-5,
...     per_device_train_batch_size=8,
...     num_train_epochs=3,
...     weight_decay=0.01,
...     logging_dir="./logs",
... )
>>> training_args = TrainingArguments(
...     output_dir="D:\workspace_python\infinity_data\model\t5-small-result",
...     learning_rate=2e-5,
...     per_device_train_batch_size=8,
...     num_train_epochs=3,
...     weight_decay=0.01,
...     logging_dir="D:\workspace_python\infinity_data\model\t5-small-result\logs",
... )
>>>
>>>     model=model,
  File "<stdin>", line 1
    model=model,
IndentationError: unexpected indent
>>>     args=training_args,
  File "<stdin>", line 1
    args=training_args,
IndentationError: unexpected indent
>>>     train_dataset=tokenized_dataset["train"],
  File "<stdin>", line 1
    train_dataset=tokenized_dataset["train"],
IndentationError: unexpected indent
>>>     eval_dataset=tokenized_dataset["validation"],
  File "<stdin>", line 1
    eval_dataset=tokenized_dataset["validation"],
IndentationError: unexpected indent
>>> )
  File "<stdin>", line 1
    )
    ^
SyntaxError: unmatched ')'
>>>
>>>
>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=tokenized_dataset["train"],
...     eval_dataset=tokenized_dataset["validation"],
... )
Traceback (most recent call last):
  File "<stdin>", line 2, in <module>
NameError: name 'model' is not defined
>>> import os
>>> os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
>>>
>>> from datasets import load_dataset
>>>
>>> # 指定版本（如 "refs/convert/parquet" 是官方维护的稳定分支）
>>> dataset = load_dataset(
...     "Salesforce/wikisql",
...     trust_remote_code=True,
...     revision="refs/convert/parquet"
... )
>>> print(dataset["train"][0])  # 查看数据结构
{'phase': 1, 'question': 'Tell me what the notes are for South Australia ', 'table': {'header': ['State/territory', 'Text/background colour', 'Format', 'Current slogan', 'Current series', 'Notes'], 'page_title': '', 'page_id': '', 'types': ['text', 'text', 'text', 'text', 'text', 'text'], 'id': '1-1000181-1', 'section_title': '', 'caption': '', 'rows': [['Australian Capital Territory', 'blue/white', 'Yaa·nna', 'ACT · CELEBRATION OF A CENTURY 2013', 'YIL·00A', 'Slogan screenprinted on plate'], ['New South Wales', 'black/yellow', 'aa·nn·aa', 'NEW SOUTH WALES', 'BX·99·HI', 'No slogan on current series'], ['New South Wales', 'black/white', 'aaa·nna', 'NSW', 'CPX·12A', 'Optional white slimline series'], ['Northern Territory', 'ochre/white', 'Ca·nn·aa', 'NT  · OUTBACK AUSTRALIA', 'CB·06·ZZ', 'New series began in June 2011'], ['Queensland', 'maroon/white', 'nnn·aaa', 'QUEENSLAND · SUNSHINE STATE', '999·TLG', 'Slogan embossed on plate'], ['South Australia', 'black/white', 'Snnn·aaa', 'SOUTH AUSTRALIA', 'S000·AZD', 'No slogan on current series'], ['Victoria', 'blue/white', 'aaa·nnn', 'VICTORIA - THE PLACE TO BE', 'ZZZ·562', 'Current series will be exhausted this year']], 'name': 'table_1000181_1'}, 'sql': {'human_readable': 'SELECT Notes FROM table WHERE Current slogan = SOUTH AUSTRALIA', 'sel': 5, 'agg': 0, 'conds': {'column_index': [3], 'operator_index': [0], 'condition': ['SOUTH AUSTRALIA']}}}
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration
>>>
>>> model_name = "t5-small"
>>> tokenizer = T5Tokenizer.from_pretrained(model_name)
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
>>> model = T5ForConditionalGeneration.from_pretrained(model_name)
>>>
>>> def preprocess_function(examples):
...     inputs = [f"translate English to SQL: {q}" for q in examples["question"]]
...     # 手动构造 SQL 语句（从 sel/agg/conds 字段提取信息）
...     targets = []
...     for i in range(len(examples["sql"])):
...         sql_info = examples["sql"][i]
...         # 提取 SELECT 列名（需从 table 的 header 中映射）
...         selected_col = examples["table"][i]["header"][sql_info["sel"]]
...         # 提取 WHERE 条件（需处理多个条件）
...         conditions = []
...         for col_idx, op_idx, value in zip(sql_info["conds"]["column_index"],
...                                           sql_info["conds"]["operator_index"],
...                                           sql_info["conds"]["condition"]):
...             col_name = examples["table"][i]["header"][col_idx]
...             op = ["=", ">", "<", "LIKE"][op_idx]  # 根据 WikiSQL 的 operator 定义调整
...             conditions.append(f"{col_name} {op} {repr(value)}")
...         where_clause = " AND ".join(conditions) if conditions else ""
...         # 构造完整 SQL
...         target_sql = f"SELECT {selected_col} FROM table"
...         if where_clause:
...             target_sql += f" WHERE {where_clause}"
...         targets.append(target_sql)
...     # Tokenize 输入和输出
...     model_inputs = tokenizer(inputs, max_length=128, truncation=True)
...     labels = tokenizer(targets, max_length=128, truncation=True)
...     model_inputs["labels"] = labels["input_ids"]
...     return model_inputs
...
>>>
>>> from transformers import TrainingArguments, Trainer
>>>
>>> training_args = TrainingArguments(
...     output_dir="D:\workspace_python\infinity_data\model\t5-small-result",
...     learning_rate=2e-5,
...     per_device_train_batch_size=8,
...     num_train_epochs=3,
...     weight_decay=0.01,
...     logging_dir="D:\workspace_python\infinity_data\model\t5-small-result\logs",
... )
>>>
>>>
>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=tokenized_dataset["train"],
...     eval_dataset=tokenized_dataset["validation"],
... )
Traceback (most recent call last):
  File "<stdin>", line 4, in <module>
NameError: name 'tokenized_dataset' is not defined
>>>
>>> tokenized_dataset = dataset.map(preprocess_function, batched=True)
Map: 100%|███████████████████████████████████████████████████████████████████████| 56355/56355 [00:34<00:00, 1653.28 examples/s]
Map: 100%|█████████████████████████████████████████████████████████████████████████| 8421/8421 [00:04<00:00, 1697.14 examples/s]
Map: 100%|███████████████████████████████████████████████████████████████████████| 15878/15878 [00:10<00:00, 1585.51 examples/s]
>>> rom transformers import TrainingArguments, Trainer
  File "<stdin>", line 1
    rom transformers import TrainingArguments, Trainer
        ^
SyntaxError: invalid syntax
>>>
>>> training_args = TrainingArguments(
...     output_dir="D:\workspace_python\infinity_data\model\t5-small-result",
...     learning_rate=2e-5,
...     per_device_train_batch_size=8,
...     num_train_epochs=3,
...     weight_decay=0.01,
...     logging_dir="D:\workspace_python\infinity_data\model\t5-small-result\logs",
... )
>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=tokenized_dataset["train"],
...     eval_dataset=tokenized_dataset["validation"],
... )
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\utils\deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\trainer.py", line 700, in __init__
    os.makedirs(self.args.output_dir, exist_ok=True)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\os.py", line 225, in makedirs
    mkdir(name, mode)
OSError: [WinError 123] 文件名、目录名或卷标语法不正确。: 'D:\\workspace_python\\infinity_data\\model\t5-small-result'
>>> training_args = TrainingArguments(
...     output_dir=rf"D:\workspace_python\infinity_data\model\t5-small-result",
...     learning_rate=2e-5,
...     per_device_train_batch_size=8,
...     num_train_epochs=3,
...     weight_decay=0.01,
...     logging_dir=rf"D:\workspace_python\infinity_data\model\t5-small-result\logs",
... )
>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=tokenized_dataset["train"],
...     eval_dataset=tokenized_dataset["validation"],
... )
>>> trainer.train()
  0%|                                                                                                 | 0/21135 [00:00<?, ?it/s]C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\torch\utils\data\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.
  warnings.warn(warn_msg)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\trainer.py", line 2240, in train
    return inner_training_loop(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\trainer.py", line 2509, in _inner_training_loop
    batch_samples, num_items_in_batch = self.get_batch_samples(epoch_iterator, num_batches, args.device)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\trainer.py", line 5263, in get_batch_samples
    batch_samples.append(next(epoch_iterator))
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\accelerate\data_loader.py", line 566, in __iter__
    current_batch = next(dataloader_iter)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\torch\utils\data\dataloader.py", line 733, in __next__
    data = self._next_data()
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\torch\utils\data\dataloader.py", line 789, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\torch\utils\data\_utils\fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\data\data_collator.py", line 93, in default_data_collator
    return torch_default_data_collator(features)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\data\data_collator.py", line 159, in torch_default_data_collator
    batch[k] = torch.tensor([f[k] for f in features])
ValueError: expected sequence of length 30 at dim 1 (got 20)
>>> def preprocess_function(examples):
...     inputs = [f"translate English to SQL: {q}" for q in examples["question"]]
...
>>>     # 构造 SQL 目标语句
>>>     targets = []
  File "<stdin>", line 1
    targets = []
IndentationError: unexpected indent
>>>     for i in range(len(examples["sql"])):
  File "<stdin>", line 1
    for i in range(len(examples["sql"])):
IndentationError: unexpected indent
>>>         sql_info = examples["sql"][i]
  File "<stdin>", line 1
    sql_info = examples["sql"][i]
IndentationError: unexpected indent
>>>         selected_col = examples["table"][i]["header"][sql_info["sel"]]
  File "<stdin>", line 1
    selected_col = examples["table"][i]["header"][sql_info["sel"]]
IndentationError: unexpected indent
>>>
>>>         # 处理 WHERE 条件（限制最多 2 个条件）
>>>         conditions = []
  File "<stdin>", line 1
    conditions = []
IndentationError: unexpected indent
>>>         for col_idx, op_idx, value in zip(
  File "<stdin>", line 1
    for col_idx, op_idx, value in zip(
IndentationError: unexpected indent
>>>                 sql_info["conds"]["column_index"][:2],  # 截断条件数量
  File "<stdin>", line 1
    sql_info["conds"]["column_index"][:2],  # 截断条件数量
IndentationError: unexpected indent
>>>                 sql_info["conds"]["operator_index"][:2],
  File "<stdin>", line 1
    sql_info["conds"]["operator_index"][:2],
IndentationError: unexpected indent
>>>                 sql_info["conds"]["condition"][:2]
  File "<stdin>", line 1
    sql_info["conds"]["condition"][:2]
IndentationError: unexpected indent
>>>         ):
  File "<stdin>", line 1
    ):
IndentationError: unexpected indent
>>>             col_name = examples["table"][i]["header"][col_idx]
  File "<stdin>", line 1
    col_name = examples["table"][i]["header"][col_idx]
IndentationError: unexpected indent
>>>             op = ["=", ">", "<", "LIKE"][op_idx]
  File "<stdin>", line 1
    op = ["=", ">", "<", "LIKE"][op_idx]
IndentationError: unexpected indent
>>>             conditions.append(f"{col_name} {op} {repr(value)}")
  File "<stdin>", line 1
    conditions.append(f"{col_name} {op} {repr(value)}")
IndentationError: unexpected indent
>>>         where_clause = " AND ".join(conditions)
  File "<stdin>", line 1
    where_clause = " AND ".join(conditions)
IndentationError: unexpected indent
>>>
>>>         target_sql = f"SELECT {selected_col} FROM table"
  File "<stdin>", line 1
    target_sql = f"SELECT {selected_col} FROM table"
IndentationError: unexpected indent
>>>         if where_clause:
  File "<stdin>", line 1
    if where_clause:
IndentationError: unexpected indent
>>>             target_sql += f" WHERE {where_clause}"
  File "<stdin>", line 1
    target_sql += f" WHERE {where_clause}"
IndentationError: unexpected indent
>>>         targets.append(target_sql)
  File "<stdin>", line 1
    targets.append(target_sql)
IndentationError: unexpected indent
>>>
>>>     # 统一填充到固定长度
>>>     model_inputs = tokenizer(
  File "<stdin>", line 1
    model_inputs = tokenizer(
IndentationError: unexpected indent
>>>         inputs,
  File "<stdin>", line 1
    inputs,
IndentationError: unexpected indent
>>>         max_length=128,
  File "<stdin>", line 1
    max_length=128,
IndentationError: unexpected indent
>>>         truncation=True,
  File "<stdin>", line 1
    truncation=True,
IndentationError: unexpected indent
>>>         padding="max_length",  # 强制填充
  File "<stdin>", line 1
    padding="max_length",  # 强制填充
IndentationError: unexpected indent
>>>         return_tensors="pt"
  File "<stdin>", line 1
    return_tensors="pt"
IndentationError: unexpected indent
>>>     )
  File "<stdin>", line 1
    )
IndentationError: unexpected indent
>>>     labels = tokenizer(
  File "<stdin>", line 1
    labels = tokenizer(
IndentationError: unexpected indent
>>>         targets,
  File "<stdin>", line 1
    targets,
IndentationError: unexpected indent
>>>         max_length=128,
  File "<stdin>", line 1
    max_length=128,
IndentationError: unexpected indent
>>>         truncation=True,
  File "<stdin>", line 1
    truncation=True,
IndentationError: unexpected indent
>>>         padding="max_length",  # 强制填充
  File "<stdin>", line 1
    padding="max_length",  # 强制填充
IndentationError: unexpected indent
>>>         return_tensors="pt"
  File "<stdin>", line 1
    return_tensors="pt"
IndentationError: unexpected indent
>>>     )
  File "<stdin>", line 1
    )
IndentationError: unexpected indent
>>>     model_inputs["labels"] = labels["input_ids"]
  File "<stdin>", line 1
    model_inputs["labels"] = labels["input_ids"]
IndentationError: unexpected indent
>>>     return model_inputs
  File "<stdin>", line 1
    return model_inputs
IndentationError: unexpected indent
>>>
>>> def preprocess_function(examples):
...     inputs = [f"translate English to SQL: {q}" for q in examples["question"]]
...     # 构造 SQL 目标语句
...     targets = []
...     for i in range(len(examples["sql"])):
...         sql_info = examples["sql"][i]
...         selected_col = examples["table"][i]["header"][sql_info["sel"]]
...         # 处理 WHERE 条件（限制最多 2 个条件）
...         conditions = []
...         for col_idx, op_idx, value in zip(
...                 sql_info["conds"]["column_index"][:2],  # 截断条件数量
...                 sql_info["conds"]["operator_index"][:2],
...                 sql_info["conds"]["condition"][:2]
...         ):
...             col_name = examples["table"][i]["header"][col_idx]
...             op = ["=", ">", "<", "LIKE"][op_idx]
...             conditions.append(f"{col_name} {op} {repr(value)}")
...         where_clause = " AND ".join(conditions)
...         target_sql = f"SELECT {selected_col} FROM table"
...         if where_clause:
...             target_sql += f" WHERE {where_clause}"
...         targets.append(target_sql)
...     # 统一填充到固定长度
...     model_inputs = tokenizer(
...         inputs,
...         max_length=128,
...         truncation=True,
...         padding="max_length",  # 强制填充
...         return_tensors="pt"
...     )
...     labels = tokenizer(
...         targets,
...         max_length=128,
...         truncation=True,
...         padding="max_length",  # 强制填充
...         return_tensors="pt"
...     )
...     model_inputs["labels"] = labels["input_ids"]
...     return model_inputs
...
>>> # 4. 数据预处理
>>> tokenized_dataset = dataset.map(
...     preprocess_function,
...     batched=True,
...     batch_size=1000,  # 优化内存占用
...     remove_columns=dataset["train"].column_names
... )
Map: 100%|███████████████████████████████████████████████████████████████████████| 56355/56355 [00:36<00:00, 1555.68 examples/s]
Map: 100%|█████████████████████████████████████████████████████████████████████████| 8421/8421 [00:05<00:00, 1571.93 examples/s]
Map: 100%|███████████████████████████████████████████████████████████████████████| 15878/15878 [00:10<00:00, 1504.84 examples/s]
>>>  100%|███████████████████████████████████████████████████████████████████████| 15878/15878 [00:10<00:00, 1523.57 examples/s]
>>> data_collator = DataCollatorForSeq2Seq(
...     tokenizer=tokenizer,
...     model=model,
...     padding=True,
...     pad_to_multiple_of=8  # 优化 GPU 内存对齐
... )
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'DataCollatorForSeq2Seq' is not defined
>>>
>>> from transformers import DataCollatorForSeq2Seq
>>> tokenizer = T5Tokenizer.from_pretrained("t5-small")
>>> model = T5ForConditionalGeneration.from_pretrained("t5-small")
>>> data_collator = DataCollatorForSeq2Seq(
...     tokenizer=tokenizer,
...     model=model,
...     padding=True,
...     pad_to_multiple_of=8  # 优化 GPU 内存对齐
... )
>>> training_args = TrainingArguments(
...     output_dir=rf"D:\workspace_python\infinity_data\model\t5-small-result",
...     learning_rate=2e-5,
...     per_device_train_batch_size=8,
...     num_train_epochs=3,
...     weight_decay=0.01,
...     logging_dir=rf"D:\workspace_python\infinity_data\model\t5-small-result\logs",
...     fp16=True,                  # 启用混合精度训练
...     gradient_accumulation_steps=2,  # 梯度累积
...     dataloader_drop_last=True  # 丢弃不完整批次[3,8](@ref)
... )
>>>
>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=tokenized_dataset["train"],
...     eval_dataset=tokenized_dataset["validation"],
...     data_collator=data_collator,  # 关键修复[3,4](@ref)
...     tokenizer=tokenizer
... )
<stdin>:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  0%|                                                                                                 | 0/21135 [19:31<?, ?it/s]
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration
>>>
>>> model_name = "t5-small"
>>> tokenizer = T5Tokenizer.from_pretrained(model_name)
>>> model = T5ForConditionalGeneration.from_pretrained(model_name)
>>>
>>>
>>> def preprocess_function(examples):
...     inputs = [f"translate English to SQL: {q}" for q in examples["question"]]
...     # 构造 SQL 目标语句
...     targets = []
...     for i in range(len(examples["sql"])):
...         sql_info = examples["sql"][i]
...         selected_col = examples["table"][i]["header"][sql_info["sel"]]
...         # 处理 WHERE 条件（限制最多 2 个条件）
...         conditions = []
...         for col_idx, op_idx, value in zip(
...                 sql_info["conds"]["column_index"][:2],  # 截断条件数量
...                 sql_info["conds"]["operator_index"][:2],
...                 sql_info["conds"]["condition"][:2]
...         ):
...             col_name = examples["table"][i]["header"][col_idx]
...             op = ["=", ">", "<", "LIKE"][op_idx]
...             conditions.append(f"{col_name} {op} {repr(value)}")
...         where_clause = " AND ".join(conditions)
...         target_sql = f"SELECT {selected_col} FROM table"
...         if where_clause:
...             target_sql += f" WHERE {where_clause}"
...         targets.append(target_sql)
...     # 统一填充到固定长度
...     model_inputs = tokenizer(
...         inputs,
...         max_length=128,
...         truncation=True,
...         padding="max_length",  # 强制填充
...         return_tensors="pt"
...     )
...     labels = tokenizer(
...         targets,
...         max_length=128,
...         truncation=True,
...         padding="max_length",  # 强制填充
...         return_tensors="pt"
...     )
...     model_inputs["labels"] = labels["input_ids"]
...     return model_inputs
...
>>>
>>> tokenized_dataset = dataset.map(
...     preprocess_function,
...     batched=True,
...     batch_size=1000,  # 优化内存占用
...     remove_columns=dataset["train"].column_names
... )
>>> data_collator = DataCollatorForSeq2Seq(
...     tokenizer=tokenizer,
...     model=model,
...     padding=True,
...     pad_to_multiple_of=8  # 优化 GPU 内存对齐
... )
>>> training_args = TrainingArguments(
...     output_dir=rf"D:\workspace_python\infinity_data\model\t5-small-result",
...     learning_rate=2e-5,
...     per_device_train_batch_size=8,
...     num_train_epochs=3,
...     weight_decay=0.01,
...     logging_dir=rf"D:\workspace_python\infinity_data\model\t5-small-result\logs",
...     fp16=True,                  # 启用混合精度训练
...     gradient_accumulation_steps=2,  # 梯度累积
...     dataloader_drop_last=True  # 丢弃不完整批次[3,8](@ref)
... )
>>>
>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=tokenized_dataset["train"],
...     eval_dataset=tokenized_dataset["validation"],
...     data_collator=data_collator,  # 关键修复[3,4](@ref)
...     tokenizer=tokenizer
... )
>>>
>>> trainer.train()
  0%|                                                                                                 | 0/10566 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.
  2%|█▍                                                                                  | 176/10566 [24:21<24:06:51,  8.36s/it]Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\trainer.py", line 2240, in train
    return inner_training_loop(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\trainer.py", line 2555, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\trainer.py", line 3745, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\trainer.py", line 3810, in compute_loss
    outputs = model(**inputs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\torch\nn\modules\module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\torch\nn\modules\module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\models\t5\modeling_t5.py", line 1811, in forward
    decoder_outputs = self.decoder(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\torch\nn\modules\module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\torch\nn\modules\module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\models\t5\modeling_t5.py", line 1124, in forward
    layer_outputs = layer_module(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\torch\nn\modules\module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\torch\nn\modules\module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\models\t5\modeling_t5.py", line 729, in forward
    hidden_states = self.layer[-1](hidden_states)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\torch\nn\modules\module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\torch\nn\modules\module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\models\t5\modeling_t5.py", line 343, in forward
    forwarded_states = self.DenseReluDense(forwarded_states)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\torch\nn\modules\module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\torch\nn\modules\module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\models\t5\modeling_t5.py", line 288, in forward
    hidden_states = self.wi(hidden_states)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\torch\nn\modules\module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\torch\nn\modules\module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\torch\nn\modules\linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt
>>>
>>> dataset = load_dataset(
...     "Salesforce/wikisql",
...     trust_remote_code=True,
...     revision="refs/convert/parquet",
...     split={
...         "train": "train.shuffle(seed=42).select(range(500))",  # 随机选500条
...         "validation": "validation.shuffle(seed=42).select(range(100))"
...     }
... )
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\load.py", line 2096, in load_dataset
    ds = builder_instance.as_dataset(split=split, verification_mode=verification_mode, in_memory=keep_in_memory)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\builder.py", line 1127, in as_dataset
    datasets = map_nested(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\utils\py_utils.py", line 521, in map_nested
    mapped = [
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\utils\py_utils.py", line 522, in <listcomp>
    _single_map_nested((function, obj, batched, batch_size, types, None, True, None))
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\utils\py_utils.py", line 383, in _single_map_nested
    return function(data_struct)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\builder.py", line 1154, in _build_single_dataset
    split = Split(split)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\splits.py", line 451, in __new__
    return NamedSplitAll() if name == "all" else NamedSplit(name)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\datasets\splits.py", line 362, in __init__
    raise ValueError(f"Split name should match '{_split_re}' but got '{split_name}'.")
ValueError: Split name should match '^\w+(\.\w+)*$' but got 'train.shuffle(seed=42).select(range(500))'.
>>> dataset = load_dataset(
...     "Salesforce/wikisql",
...     trust_remote_code=True,
...     revision="refs/convert/parquet",
...     split={
...         "train": "train[:10%]",  # 前10%训练数据（约5,600条）
...         "validation": "validation[:5%]"  # 前5%验证数据（约420条）
...     }
... )
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration
>>>
>>> model_name = "t5-small"
>>> tokenizer = T5Tokenizer.from_pretrained(model_name)
>>> model = T5ForConditionalGeneration.from_pretrained(model_name)
>>> tokenized_dataset = dataset.map(
...     preprocess_function,
...     batched=True,
...     batch_size=1000,  # 优化内存占用
...     remove_columns=dataset["train"].column_names
... )
Map: 100%|█████████████████████████████████████████████████████████████████████████| 5636/5636 [00:03<00:00, 1462.13 examples/s]
Map: 100%|███████████████████████████████████████████████████████████████████████████| 421/421 [00:00<00:00, 1179.43 examples/s]
>>>  100%|███████████████████████████████████████████████████████████████████████████| 421/421 [00:00<00:00, 1192.76 examples/s]
>>> data_collator = DataCollatorForSeq2Seq(
...     tokenizer=tokenizer,
...     model=model,
...     padding=True,
...     pad_to_multiple_of=8  # 优化 GPU 内存对齐
... )
>>> training_args = TrainingArguments(
...     output_dir=rf"D:\workspace_python\infinity_data\model\t5-small-result",
...     learning_rate=2e-5,
...     per_device_train_batch_size=4,  # 减小批次大小（原为8）
...     num_train_epochs=1,            # 减少训练轮次（原为3）
...     gradient_accumulation_steps=4,  # 梯度累积补偿小批次
...     fp16=True,                     # 混合精度加速
...     dataloader_drop_last=True,
...     logging_steps=50               # 减少日志频率
... )
>>>
>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=tokenized_dataset["train"],
...     eval_dataset=tokenized_dataset["validation"],
...     data_collator=data_collator,  # 关键修复[3,4](@ref)
...     tokenizer=tokenizer
... )
  2%|█▎                                                                             | 176/10566 [12:38:24<746:12:00, 258.55s/it]
>>>
>>> trainer.train()
  5%|████▎                                                                                     | 17/353 [02:18<44:53,  8.02s/it]Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\trainer.py", line 2240, in train
    return inner_training_loop(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\trainer.py", line 2555, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\trainer.py", line 3745, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\trainer.py", line 3810, in compute_loss
    outputs = model(**inputs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\torch\nn\modules\module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\torch\nn\modules\module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\models\t5\modeling_t5.py", line 1774, in forward
    encoder_outputs = self.encoder(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\torch\nn\modules\module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\torch\nn\modules\module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\models\t5\modeling_t5.py", line 1124, in forward
    layer_outputs = layer_module(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\torch\nn\modules\module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\torch\nn\modules\module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\models\t5\modeling_t5.py", line 729, in forward
    hidden_states = self.layer[-1](hidden_states)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\torch\nn\modules\module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\torch\nn\modules\module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\models\t5\modeling_t5.py", line 343, in forward
    forwarded_states = self.DenseReluDense(forwarded_states)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\torch\nn\modules\module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\torch\nn\modules\module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\models\t5\modeling_t5.py", line 290, in forward
    hidden_states = self.dropout(hidden_states)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\torch\nn\modules\module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\torch\nn\modules\module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\torch\nn\modules\dropout.py", line 70, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\torch\nn\functional.py", line 1425, in dropout
    _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
KeyboardInterrupt
>>> dataset = load_dataset(
...     "Salesforce/wikisql",
...     trust_remote_code=True,
...     revision="refs/convert/parquet",
...     split={
...         "train": "train[:5%]",  # 前10%训练数据（约5,600条）
...         "validation": "validation[:5%]"  # 前5%验证数据（约420条）
...     }
... )
>>>
>>> tokenized_dataset = dataset.map(
...     preprocess_function,
...     batched=True,
...     batch_size=1000,  # 优化内存占用
...     remove_columns=dataset["train"].column_names
... )
Map: 100%|█████████████████████████████████████████████████████████████████████████| 2818/2818 [00:01<00:00, 1598.85 examples/s]
Map: 100%|████████████████████████████████████████████████████████████████████████████| 421/421 [00:00<00:00, 989.19 examples/s]
>>>  100%|████████████████████████████████████████████████████████████████████████████| 421/421 [00:00<00:00, 998.56 examples/s]
>>>
>>>
>>> data_collator = DataCollatorForSeq2Seq(
...     tokenizer=tokenizer,
...     model=model,
...     padding=True,
...     pad_to_multiple_of=8  # 优化 GPU 内存对齐
... )
>>>
>>> training_args = TrainingArguments(
...     output_dir=rf"D:\workspace_python\infinity_data\model\t5-small-result",
...     learning_rate=2e-5,
...     per_device_train_batch_size=4,  # 减小批次大小（原为8）
...     num_train_epochs=1,            # 减少训练轮次（原为3）
...     gradient_accumulation_steps=4,  # 梯度累积补偿小批次
...     fp16=True,                     # 混合精度加速
...     dataloader_drop_last=True,
...     logging_steps=50               # 减少日志频率
... )
>>>
>>>
>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=tokenized_dataset["train"],
...     eval_dataset=tokenized_dataset["validation"],
...     data_collator=data_collator,  # 关键修复[3,4](@ref)
...     tokenizer=tokenizer
... )
>>>
>>> trainer.train()
{'loss': 6.1166, 'grad_norm': 41.3803596496582, 'learning_rate': 1.4431818181818182e-05, 'epoch': 0.28}
{'loss': 1.9281, 'grad_norm': 3.599303722381592, 'learning_rate': 8.750000000000001e-06, 'epoch': 0.57}
{'loss': 1.3145, 'grad_norm': 4.593541145324707, 'learning_rate': 3.0681818181818186e-06, 'epoch': 0.85}
{'train_runtime': 1436.2773, 'train_samples_per_second': 1.962, 'train_steps_per_second': 0.123, 'train_loss': 2.828643874688582, 'epoch': 1.0}
100%|█████████████████████████████████████████████████████████████████████████████████████████| 176/176 [23:56<00:00,  8.16s/it]
TrainOutput(global_step=176, training_loss=2.828643874688582, metrics={'train_runtime': 1436.2773, 'train_samples_per_second': 1.962, 'train_steps_per_second': 0.123, 'total_flos': 95280628236288.0, 'train_loss': 2.828643874688582, 'epoch': 1.0})
>>> model_save_path = rf"D:\workspace_python\infinity_data\model\t5-small"
>>> torch.save(model.state_dict(), model_save_path)
  5%|████▏                                                                                 | 17/353 [32:44<10:47:08, 115.56s/it]
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'torch' is not defined
>>> import torch
>>> import torch
>>> model_save_path = rf"D:\workspace_python\infinity_data\model\t5-small"
>>> torch.save(model.state_dict(), model_save_path)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\torch\serialization.py", line 964, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\torch\serialization.py", line 828, in _open_zipfile_writer
    return container(name_or_buffer)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\torch\serialization.py", line 792, in __init__
    torch._C.PyTorchFileWriter(
RuntimeError: File D:\workspace_python\infinity_data\model\t5-small cannot be opened.
>>> model_save_path = rf"D:\workspace_python\infinity_data\model\t5-small-trained"
>>> torch.save(model.state_dict(), model_save_path)
>>> import torch
>>> model_save_path = rf"D:\workspace_python\infinity_data\model\t5_finetuned"
>>> torch.save(model.state_dict(), model_save_path)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\torch\serialization.py", line 964, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\torch\serialization.py", line 828, in _open_zipfile_writer
    return container(name_or_buffer)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\torch\serialization.py", line 792, in __init__
    torch._C.PyTorchFileWriter(
RuntimeError: File D:\workspace_python\infinity_data\model\t5_finetuned cannot be opened.
>>> model_save_path = rf"D:\workspace_python\infinity_data\model\t5_finetuned\t5_model"
>>> torch.save(model.state_dict(), model_save_path)
>>> model.save_pretrained(model_save_path)
>>> tokenizer.save_pretrained(model_save_path)
('D:\\workspace_python\\infinity_data\\model\\t5_finetuned\\t5_model\\tokenizer_config.json', 'D:\\workspace_python\\infinity_data\\model\\t5_finetuned\\t5_model\\special_tokens_map.json', 'D:\\workspace_python\\infinity_data\\model\\t5_finetuned\\t5_model\\spiece.model', 'D:\\workspace_python\\infinity_data\\model\\t5_finetuned\\t5_model\\added_tokens.json')
>>> model_save_path = rf"D:\workspace_python\infinity_data\model\t5-small-finetuned"
>>> model.save_pretrained(model_save_path)
>>> tokenizer.save_pretrained(model_save_path)
('D:\\workspace_python\\infinity_data\\model\\t5-small-finetuned\\tokenizer_config.json', 'D:\\workspace_python\\infinity_data\\model\\t5-small-finetuned\\special_tokens_map.json', 'D:\\workspace_python\\infinity_data\\model\\t5-small-finetuned\\spiece.model', 'D:\\workspace_python\\infinity_data\\model\\t5-small-finetuned\\added_tokens.json')
>>> eval_results = trainer.evaluate()
C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\torch\utils\data\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.
  warnings.warn(warn_msg)
100%|███████████████████████████████████████████████████████████████████████████████████████████| 52/52 [00:43<00:00,  1.19it/s]
>>> print(f"Validation Loss: {eval_results['eval_loss']:.4f}")
Validation Loss: 0.7354
>>> from transformers import T5ForConditionalGeneration, T5Tokenizer
>>> model = T5ForConditionalGeneration.from_pretrained(rf"D:\workspace_python\infinity_data\model\t5-small-result")
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\modeling_utils.py", line 309, in _wrapper
    return func(*args, **kwargs)
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\modeling_utils.py", line 4421, in from_pretrained
    checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(
  File "C:\Users\ASUS\Anaconda3\envs\py39_test\lib\site-packages\transformers\modeling_utils.py", line 975, in _get_resolved_checkpoint_files
    raise EnvironmentError(
OSError: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory D:\workspace_python\infinity_data\model\t5-small-result.
>>> tokenizer = T5Tokenizer.from_pretrained("t5-small")  # 或使用训练时保存的分词器
>>> model = T5ForConditionalGeneration.from_pretrained(rf"D:\workspace_python\infinity_data\model\t5-small-result\checkpoint-176")
>>> tokenizer = T5Tokenizer.from_pretrained("t5-small")  # 或使用训练时保存的分词器
>>> def generate_sql(question, max_length=100, num_beams=4):
...     inputs = tokenizer(question, return_tensors="pt", padding=True, truncation=True)
...     outputs = model.generate(
...         inputs["input_ids"],
...         max_length=max_length,
...         num_beams=num_beams,
...         early_stopping=True
...     )
...     return tokenizer.decode(outputs[0], skip_special_tokens=True)
...
>>>
>>> question = "What is the average salary of employees in the 'Engineering' department?"
>>> sql_query = generate_sql(question)
>>> print("Generated SQL:", sql_query)
Generated SQL: Was ist der Durchschnittsgehalt der Beschäftigten im Bereich "Engineering"?
>>>
>>>